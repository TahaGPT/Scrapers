{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup as bsp\n",
    "from selenium import webdriver\n",
    "from seleniumwire import webdriver  # blinker == 1.7.0\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.edge.service import Service as EdgeService\n",
    "from selenium.webdriver.edge.options import Options as EdgeOptions\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from fake_useragent import UserAgent\n",
    "from googlesearch import search\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "base = 'https://www.amazon.com'\n",
    "thirdWheel = '/s?k='\n",
    "date_pattern = r\"\\b(January|February|March|April|May|June|July|August|September|October|November|December) \\d{1,2}, \\d{4}\\b\"\n",
    "number_pattern = r\"\\d+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session = requests.Session()\n",
    "# session.headers.update({\"User-Agent\": UserAgent().random})\n",
    "driver = webdriver.Chrome()\n",
    "# driver = webdriver.Firefox(service = service, options = options)\n",
    "driver.get(base)\n",
    "#   It will scroll to right above the footer of the page then scoll to the top then back to the bottom untill there is no new items being loaded\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(10) \n",
    "html = driver.page_source\n",
    "driver.quit()          \n",
    "soup = bsp(html, 'lxml')\n",
    "boxes = soup.find_all('a', class_ = 'a-link-normal _fluid-quad-image-label-v2_style_centerImage__30wh- aok-block image-window')\n",
    "print(boxes)\n",
    "links = []\n",
    "urls = []\n",
    "categ = []\n",
    "for box in boxes:\n",
    "    actual = box.get('href')\n",
    "    url = base + actual\n",
    "    print(\"Url : \", url)\n",
    "    urls.append(url)\n",
    "    som = box.get('aria-label')\n",
    "    link = som.replace(' ', '+')\n",
    "    link = base + thirdWheel + link\n",
    "    print(\"Links : \", link)\n",
    "    links.append(link)\n",
    "    categ.append(som)\n",
    "    print(som)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_proxies():\n",
    "    proxy_list_url = 'https://free-proxy-list.net/'\n",
    "    response = requests.get(proxy_list_url)\n",
    "    soup = bsp(response.text, 'html.parser')\n",
    "    proxy_data = []\n",
    "    rows = soup.find_all('tr')[1:]\n",
    "    for row in rows:\n",
    "        columns = row.find_all('td')\n",
    "        if len(columns) >= 8:\n",
    "            ip_address = columns[0].text.strip()\n",
    "            google_enabled = columns[5].text.strip().lower() == 'yes'\n",
    "            https_enabled = columns[6].text.strip().lower() == 'yes'\n",
    "            last_checked = columns[7].text.strip()\n",
    "            if (last_checked.endswith('mins ago') and int(last_checked.split(' ')[0]) < 15) or last_checked.endswith('hours ago'):\n",
    "                if google_enabled or https_enabled:\n",
    "                    proxy_data.append({'ip_address': ip_address, 'google_enabled': google_enabled, 'https_enabled': https_enabled})\n",
    "\n",
    "    return proxy_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_user_agent(proxy):\n",
    "    if proxy:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "            'http': f'http://{proxy}',\n",
    "            'https': f'https://{proxy}'\n",
    "        }\n",
    "    else:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "        }\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = []\n",
    "categories = []\n",
    "cat = 0\n",
    "me = False\n",
    "# proxies = get_valid_proxies()\n",
    "for url in urls:\n",
    "    total_samples = 0\n",
    "    myurl = url\n",
    "    while myurl:\n",
    "        # proxy = proxies[total_samples % len(proxies)] if proxies else None\n",
    "        options = {\n",
    "            'headers' : {\n",
    "                \"User-Agent\": UserAgent().random\n",
    "            }\n",
    "        }\n",
    "        print(\"VISITING : \", myurl)\n",
    "        # sdsd = Options()\n",
    "        # sdsd.add_argument(\"--headless\")  # Run in background\n",
    "        # sdsd.add_argument(\"--disable-gpu\")\n",
    "        # sdsd.add_argument(\"--no-sandbox\")\n",
    "        # sdsd.add_argument(\"--disable-dev-shm-usage\")\n",
    "        driver = webdriver.Chrome(seleniumwire_options = options)\n",
    "        # driver = webdriver.Firefox()\n",
    "        driver.get(myurl)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "        time.sleep(10)\n",
    "        html = driver.page_source\n",
    "        driver.quit()\n",
    "        # headers = rotate_user_agent(proxy)\n",
    "        # response = req.get(myurl, headers = headers)\n",
    "        # session = requests.Session()\n",
    "        # session.headers.update({\"User-Agent\": UserAgent().random})\n",
    "        # response = req.get(myurl, headers = {\"User-Agent\": UserAgent().random})\n",
    "        # if response.status_code == 200:\n",
    "            # soup = bsp(response.text, 'lxml')\n",
    "        soup = bsp(html, 'lxml')\n",
    "        # print(soup)\n",
    "        productes = soup.find_all('a', class_ = 'a-link-normal s-line-clamp-4 s-link-style a-text-normal')\n",
    "        \n",
    "        if not productes:\n",
    "            productes = soup.find_all('a' , class_= 'a-link-normal s-line-clamp-2 s-link-style a-text-normal')\n",
    "        print(productes)\n",
    "        for product in productes:\n",
    "            product = base + product.get('href')\n",
    "            products.append(product)\n",
    "            if cat in range(len(categ)):\n",
    "                categories.append(categ[cat])\n",
    "            print(product)\n",
    "            csv_file = 'Amazon.csv'\n",
    "            info = {\"Products\": products, \"Categories\": categories}\n",
    "            new_df = pd.DataFrame(info)\n",
    "            if not me:\n",
    "                new_df.to_csv(csv_file, mode='a', index = False)\n",
    "                me = True\n",
    "            else:\n",
    "                new_df.to_csv(csv_file, mode='a', header=False, index = False)\n",
    "            print(\"|||||||||||||||||||||||||||| Data appended successfully with continued index. |||||||||||||||||||||||||||\")\n",
    "        \n",
    "        total_samples += 1\n",
    "        nextP = soup.find('a', class_ = 's-pagination-item s-pagination-next s-pagination-button s-pagination-button-accessibility s-pagination-separator')\n",
    "        if nextP:\n",
    "            myurl = base + nextP.get('href')\n",
    "        if total_samples >= 3:\n",
    "            break\n",
    "    cat += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = {\n",
    "    \"Main_Category\": [],\n",
    "    \"Title\": [],\n",
    "    \"AvgRating\": [],\n",
    "    \"RatingNo\": [],\n",
    "    \"Features\": [[]],\n",
    "    \"Description\": [[]],\n",
    "    \"Price\": [],\n",
    "    \"Images\": [[]],\n",
    "    \"Videos\": [[]],\n",
    "    \"Store\": [],\n",
    "    \"Categories\": [],\n",
    "    \"Details\": [[]],\n",
    "    \"ParentAsin\": [],\n",
    "    \"BoughtTogether\": [[]]\n",
    "}\n",
    "\n",
    "comment = {\n",
    "    \n",
    "}\n",
    "\n",
    "csv_file = 'G:\\\\Amazon.csv'\n",
    "df = pd.readcsv(csv_file)\n",
    "samples = 0\n",
    "for row in df.iterrows():\n",
    "    options = {\n",
    "            'headers' : {\n",
    "                \"User-Agent\": UserAgent().random\n",
    "            }\n",
    "        }\n",
    "    link = row['Products']\n",
    "    print(\"VISITING : \", link)\n",
    "    driver = webdriver.Chrome(seleniumwire_options = options)\n",
    "    driver.get(link)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "    time.sleep(10)\n",
    "    html = driver.page_source\n",
    "    driver.quit()\n",
    "    soup = bsp(html, 'lxml')\n",
    "\n",
    "    feat = []\n",
    "    img = []\n",
    "    vid = []\n",
    "    det = []\n",
    "\n",
    "    # extracting Category\n",
    "    category = row['Categories']\n",
    "\n",
    "    # extracting Title\n",
    "    try:\n",
    "        titleBoard = soup.find('span', id = 'productTitle')\n",
    "        title = titleBoard.text.strip()\n",
    "    except:\n",
    "        title = \"\"\n",
    "\n",
    "    # extracting ratings\n",
    "    try:\n",
    "        ratingBoard = soup.find('div', id = 'averageCustomerReviews')\n",
    "        # extracting average rating\n",
    "        avgRatingBoard = ratingBoard.find('span', class_ = 'a-size-base a-color-base')\n",
    "        avgRating = avgRatingBoard.text.strip()\n",
    "        # extracting rating number\n",
    "        ratingNoBoard = ratingBoard.find('span', id = 'acrCustomerReviewText')\n",
    "        ratingNo = ratingNoBoard.text.strip()\n",
    "        junk = \" ratings\"\n",
    "        ratingNo = ratingNo.replace(junk, \"\")\n",
    "    except:\n",
    "        avgRating = \"\"\n",
    "        ratingNo = \"\"\n",
    "\n",
    "    # extracting price\n",
    "    try:\n",
    "        priceBoard = soup.find('div', class_ = 'a-section a-spacing-none aok-align-center aok-relative').find('span', class_ = 'aok-offscreen')\n",
    "        price = priceBoard.text.strip()[0:5]\n",
    "    except:\n",
    "        price = \"\"\n",
    "\n",
    "    # exracting media\n",
    "    try:\n",
    "        mediaBoard = soup.find('ul', class_ = 'a-unordered-list a-nostyle a-button-list a-vertical a-spacing-top-micro regularAltImageViewLayout')\n",
    "        if not mediaBoard:\n",
    "            mediaBoard = soup.find('ul', class_ = 'a-unordered-list a-nostyle a-button-list a-vertical a-spacing-top-micro gridAltImageViewLayoutIn1x7')\n",
    "        media = mediaBoard.find_all('img')\n",
    "        images = [img['src'] for img in media]\n",
    "        vids = images[-1]\n",
    "        images = images[0:-1]\n",
    "    except:\n",
    "        images = []\n",
    "        vids = []\n",
    "    \n",
    "\n",
    "    # extracting the store\n",
    "    try:\n",
    "        storeBoard = soup.find('tr', class_ = 'a-spacing-small po-brand').find('td', class_ = 'a-span9')\n",
    "        store = storeBoard.text.strip()\n",
    "    except:\n",
    "        store = \"\"\n",
    "\n",
    "    # extracting features\n",
    "    try:\n",
    "        featuresBoard = soup.find('ul', class_ = 'a-unordered-list a-vertical a-spacing-mini').find_all('il')\n",
    "        features = [feature.text.strip() for feature in featuresBoard]\n",
    "    except:\n",
    "        features = []\n",
    "        \n",
    "    # extracting details\n",
    "    try:\n",
    "        detailsBoard = soup.find('div', class_ = 'a-expander-content a-expander-section-content a-section-expander-inner').find('table', class_ = 'a-keyvalue prodDetTable').find_all('tr')\n",
    "    except:\n",
    "        try:\n",
    "            detailsBoard = soup.find('div', id = 'detailBullets_feature_div').find_all('li')\n",
    "        except:\n",
    "            try:\n",
    "                detailsBoard = soup.find('table', id = 'productDetails_detailBullets_sections1').find_all('tr')\n",
    "            except:    \n",
    "                try:\n",
    "                    detailsBoard = soup.find('ul', class_ = 'a-unordered-list a-nostyle a-vertical a-spacing-none detail-bullet-list').find_all('tr')\n",
    "                except:\n",
    "                    detailsBoard = []\n",
    "                    \n",
    "    # extracting asin\n",
    "    try:\n",
    "        asinBoard = soup.find('tr', text=lambda x: x.strip() == 'ASIN', class_='prodDetSectionEntry')\n",
    "        asin = asinBoard.text.strip()\n",
    "    except:\n",
    "        asin = \"\"\n",
    "    \n",
    "    # extracting description\n",
    "    try:\n",
    "        descriptionBoard = soup.find('ul', id = 'a-nostyle').find_all('div', class_ = 'a-fixed-left-grid-col a-col-right')\n",
    "        descriptionBoardB = soup.find('table', class_ = 'a-normal a-spacing-micro')\n",
    "        descriptions = descriptionBoardB.find_all('td', class_ = 'a-span9')\n",
    "        description = [dec.text.strip() for dec in descriptions]\n",
    "    except:\n",
    "        description = \"\"\n",
    "\n",
    "    # extracting bought together\n",
    "    boughtTogetherBoard = soup.find('div', id = 'a-cardui _c3AtZ_new-thumbnail-box_1W9Ku _c3AtZ_two-item-thumbnail-box_7kF95')\n",
    "    if not boughtTogetherBoard:\n",
    "        boughtTogetherBoard = soup.find('div', class_ = 'a-cardui _p13n-desktop-sims-fbt_fbt-desktop_new-thumbnail-box__36bD3 _p13n-desktop-sims-fbt_fbt-desktop_two-item-thumbnail-box__jV2am')\n",
    "    if not boughtTogetherBoard:\n",
    "        boughtTogetherBoard = soup.find('div', class_ = 'a-cardui _p13n-desktop-sims-fbt_fbt-desktop_new-thumbnail-box__36bD3')\n",
    "    boughtTogetherS = boughtTogetherBoard.find_all('a', class_ = 'a-link-normal a-text-normal', href = True)\n",
    "    if not boughtTogetherS:\n",
    "        boughtTogetherS = boughtTogetherBoard.find_all('a', class_ = 'a-link-normal', href = True)        \n",
    "    boughtTogether = [base + link['href'] for link in boughtTogetherS]\n",
    "\n",
    "    # extracting further categories\n",
    "    catgoriesBoard = soup.find_all('div', id= 'tp-inline-twister-dim-values-container')\n",
    "    categoriesBoard = categoriesBoard[1]\n",
    "\n",
    "\n",
    "    ############################################################################################################################################################\n",
    "    # extracting reviews\n",
    "    # reviews from US\n",
    "    reviewsUS = soup.find_all('ul', id = 'x`t').find_all('li')\n",
    "    reviewsGlobal = soup.find_all('ul', id = 'cm-cr-global-review-list').find_all('li')\n",
    "    reviews = reviewsUS + reviewsGlobal\n",
    "    for review in reviews:\n",
    "        # extracting rating\n",
    "        reviewRatingBoard = review.find('i', class_ = 'review-star-rating').find('span', class_ = 'fa-icon-alt')\n",
    "        reviewRating = reviewRatingBoard.text[0:3]\n",
    "\n",
    "        # extracting title\n",
    "        reviewTitleBoard = review.find('a', class_ = 'a-size-base a-link-normal review-title a-color-base review-title-content a-text-bold')\n",
    "        reviewTitles = reviewTitleBoard.find_all('span')\n",
    "        reviewTitle = reviewTitles[-1].text.strip()\n",
    "\n",
    "        # extracting Text\n",
    "        reviewTextBoard = review.find('div', class_ = 'a-expander-content reviewText review-text-content a-expander-partial-collapse-content')\n",
    "        reviewText = reviewTextBoard.text.strip()\n",
    "\n",
    "        # extracting images\n",
    "        reviewImages = review.find('div', class_ = 'review-image-tile-section').find_all('img')\n",
    "        reviewImages = [img['src'] for img in reviewImages]\n",
    "\n",
    "        # extracting ASIN\n",
    "        reviewAsin = asin\n",
    "\n",
    "        # extracting parent ASIN\n",
    "        reviewParentAsin = asin\n",
    "\n",
    "        # eaxtracting username\n",
    "        username = review.find('span', class_ = 'a-profile-name').text.strip()\n",
    "\n",
    "        # extracting date\n",
    "        reviewTimeBoard = review.find('span', class_ = 'a-size-base a-color-secondary review-date')\n",
    "        reviewTimes = reviewTimeBoard.text\n",
    "        reviewTimes = re.search(date_pattern, reviewTimes)\n",
    "        reviewTiming = datetime.strptime(reviewTimes, \"%B %d, %Y\")\n",
    "        reviewTime = int(reviewTiming.timestamp())\n",
    "\n",
    "        verified = False\n",
    "        if review.find('span', class_ = 'a-size-mini a-color-state a-text-bold'):\n",
    "            verified = True\n",
    "\n",
    "\n",
    "        reviewHelpful = review.find('span', class_ = 'a-size-base a-color-tertiary cr-vote-text')\n",
    "        reviewHelpful = reviewHelpful.text\n",
    "        haha = \"One\"\n",
    "        helpful = 0\n",
    "        Helpfulness = re.search(number_pattern, reviewHelpful)\n",
    "        if Helpfulness:\n",
    "            helpful = int(Helpfulness.group())\n",
    "        elif haha in reviewHelpful:\n",
    "            helpful = 1\n",
    "\n",
    "\n",
    "    # reviews from all over the globe\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the date strings\n",
    "date_str1 = \"June 22, 2016\"\n",
    "date_str2 = \"September 30, 2023\"\n",
    "\n",
    "# Convert the date strings to datetime objects\n",
    "date1 = datetime.strptime(date_str1, \"%B %d, %Y\")\n",
    "date2 = datetime.strptime(date_str2, \"%B %d, %Y\")\n",
    "\n",
    "# Compare the dates\n",
    "if date1 > date2:\n",
    "    print(f\"{date_str1} is greater than {date_str2}\")\n",
    "else:\n",
    "    print(f\"{date_str1} is not greater than {date_str2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date string in the format \"Month Day, Year\"\n",
    "date_str = \"January 17, 2025\"\n",
    "\n",
    "# Parse the string into a datetime object\n",
    "date_obj = datetime.strptime(date_str, \"%B %d, %Y\")\n",
    "\n",
    "# Convert to Unix time\n",
    "unix_time = int(date_obj.timestamp())\n",
    "\n",
    "print(f\"Unix Time: {unix_time}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example statement\n",
    "statement = \"The item costs 700 dollars, and the discount is.\"\n",
    "\n",
    "# Regular expression to match one or more digits\n",
    "number_pattern = r\"\\d+\"\n",
    "\n",
    "# Find the first number in the statement\n",
    "match = re.search(number_pattern, statement)\n",
    "\n",
    "if match:\n",
    "    number = int(match.group())  # Convert the matched number to an integer\n",
    "    print(f\"Extracted number: {number}\")\n",
    "else:\n",
    "    print(\"No number found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
