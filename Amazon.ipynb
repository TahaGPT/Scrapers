{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "from bs4 import BeautifulSoup as bsp\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from seleniumwire import webdriver  # blinker == 1.7.0\n",
    "import requests\n",
    "from selenium.webdriver.edge.service import Service as EdgeService\n",
    "from selenium.webdriver.edge.options import Options as EdgeOptions\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from fake_useragent import UserAgent\n",
    "from googlesearch import search\n",
    "\n",
    "base = 'https://www.amazon.com'\n",
    "thirdWheel = '/s?k='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session = requests.Session()\n",
    "# session.headers.update({\"User-Agent\": UserAgent().random})\n",
    "driver = webdriver.Chrome()\n",
    "# driver = webdriver.Firefox(service = service, options = options)\n",
    "driver.get(base)\n",
    "#   It will scroll to right above the footer of the page then scoll to the top then back to the bottom untill there is no new items being loaded\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(10) \n",
    "html = driver.page_source\n",
    "driver.quit()          \n",
    "soup = bsp(html, 'lxml')\n",
    "boxes = soup.find_all('a', class_ = 'a-link-normal _fluid-quad-image-label-v2_style_centerImage__30wh- aok-block image-window')\n",
    "print(boxes)\n",
    "links = []\n",
    "urls = []\n",
    "categ = []\n",
    "for box in boxes:\n",
    "    actual = box.get('href')\n",
    "    url = base + actual\n",
    "    print(\"Url : \", url)\n",
    "    urls.append(url)\n",
    "    som = box.get('aria-label')\n",
    "    link = som.replace(' ', '+')\n",
    "    link = base + thirdWheel + link\n",
    "    print(\"Links : \", link)\n",
    "    links.append(link)\n",
    "    categ.append(som)\n",
    "    print(som)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_proxies():\n",
    "    proxy_list_url = 'https://free-proxy-list.net/'\n",
    "    response = requests.get(proxy_list_url)\n",
    "    soup = bsp(response.text, 'html.parser')\n",
    "    proxy_data = []\n",
    "    rows = soup.find_all('tr')[1:]\n",
    "    for row in rows:\n",
    "        columns = row.find_all('td')\n",
    "        if len(columns) >= 8:\n",
    "            ip_address = columns[0].text.strip()\n",
    "            google_enabled = columns[5].text.strip().lower() == 'yes'\n",
    "            https_enabled = columns[6].text.strip().lower() == 'yes'\n",
    "            last_checked = columns[7].text.strip()\n",
    "            if (last_checked.endswith('mins ago') and int(last_checked.split(' ')[0]) < 15) or last_checked.endswith('hours ago'):\n",
    "                if google_enabled or https_enabled:\n",
    "                    proxy_data.append({'ip_address': ip_address, 'google_enabled': google_enabled, 'https_enabled': https_enabled})\n",
    "\n",
    "    return proxy_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_user_agent(proxy):\n",
    "    if proxy:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "            'http': f'http://{proxy}',\n",
    "            'https': f'https://{proxy}'\n",
    "        }\n",
    "    else:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "        }\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = []\n",
    "categories = []\n",
    "cat = 0\n",
    "me = False\n",
    "# proxies = get_valid_proxies()\n",
    "for url in urls:\n",
    "    total_samples = 0\n",
    "    myurl = url\n",
    "    while myurl:\n",
    "        # proxy = proxies[total_samples % len(proxies)] if proxies else None\n",
    "        options = {\n",
    "            'headers' : {\n",
    "                \"User-Agent\": UserAgent().random\n",
    "            }\n",
    "        }\n",
    "        print(\"VISITING : \", myurl)\n",
    "        # sdsd = Options()\n",
    "        # sdsd.add_argument(\"--headless\")  # Run in background\n",
    "        # sdsd.add_argument(\"--disable-gpu\")\n",
    "        # sdsd.add_argument(\"--no-sandbox\")\n",
    "        # sdsd.add_argument(\"--disable-dev-shm-usage\")\n",
    "        driver = webdriver.Chrome(seleniumwire_options = options)\n",
    "        # driver = webdriver.Firefox()\n",
    "        driver.get(myurl)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "        time.sleep(10)\n",
    "        html = driver.page_source\n",
    "        driver.quit()\n",
    "        # headers = rotate_user_agent(proxy)\n",
    "        # response = req.get(myurl, headers = headers)\n",
    "        # session = requests.Session()\n",
    "        # session.headers.update({\"User-Agent\": UserAgent().random})\n",
    "        # response = req.get(myurl, headers = {\"User-Agent\": UserAgent().random})\n",
    "        # if response.status_code == 200:\n",
    "            # soup = bsp(response.text, 'lxml')\n",
    "        soup = bsp(html, 'lxml')\n",
    "        # print(soup)\n",
    "        productes = soup.find_all('a', class_ = 'a-link-normal s-line-clamp-4 s-link-style a-text-normal')\n",
    "        \n",
    "        if not productes:\n",
    "            productes = soup.find_all('a' , class_= 'a-link-normal s-line-clamp-2 s-link-style a-text-normal')\n",
    "        print(productes)\n",
    "        for product in productes:\n",
    "            product = base + product.get('href')\n",
    "            products.append(product)\n",
    "            if cat in range(len(categ)):\n",
    "                categories.append(categ[cat])\n",
    "            print(product)\n",
    "            csv_file = 'Amazon.csv'\n",
    "            info = {\"Products\": products, \"Categories\": categories}\n",
    "            new_df = pd.DataFrame(info)\n",
    "            if not me:\n",
    "                new_df.to_csv(csv_file, mode='a', index = False)\n",
    "                me = True\n",
    "            else:\n",
    "                new_df.to_csv(csv_file, mode='a', header=False, index = False)\n",
    "            print(\"|||||||||||||||||||||||||||| Data appended successfully with continued index. |||||||||||||||||||||||||||\")\n",
    "        \n",
    "        total_samples += 1\n",
    "        nextP = soup.find('a', class_ = 's-pagination-item s-pagination-next s-pagination-button s-pagination-button-accessibility s-pagination-separator')\n",
    "        if nextP:\n",
    "            myurl = base + nextP.get('href')\n",
    "        if total_samples >= 3:\n",
    "            break\n",
    "    cat += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = {\n",
    "    \"Main_Category\": [],\n",
    "    \"Title\": [],\n",
    "    \"AvgRating\": [],\n",
    "    \"RatingNo\": [],\n",
    "    \"Features\": [[]],\n",
    "    # \"Description\": [[]],\n",
    "    \"Price\": [],\n",
    "    \"Images\": [[]],\n",
    "    \"Videos\": [[]],\n",
    "    \"Store\": [],\n",
    "    # \"Categories\": [],\n",
    "    \"Details\": [[]],\n",
    "    # \"ParentAsin\": [],\n",
    "    # \"BoughtTogether\": [[]]\n",
    "}\n",
    "\n",
    "comment = {\n",
    "    \n",
    "}\n",
    "\n",
    "csv_file = 'G:\\\\Amazon.csv'\n",
    "df = pd.readcsv(csv_file)\n",
    "samples = 0\n",
    "for row in df.iterrows():\n",
    "    options = {\n",
    "            'headers' : {\n",
    "                \"User-Agent\": UserAgent().random\n",
    "            }\n",
    "        }\n",
    "    link = row['Products']\n",
    "    print(\"VISITING : \", link)\n",
    "    driver = webdriver.Chrome(seleniumwire_options = options)\n",
    "    driver.get(link)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "    time.sleep(10)\n",
    "    html = driver.page_source\n",
    "    driver.quit()\n",
    "    soup = bsp(html, 'lxml')\n",
    "\n",
    "    feat = []\n",
    "    img = []\n",
    "    vid = []\n",
    "\n",
    "\n",
    "    # extracting Category\n",
    "    category = row['Categories']\n",
    "\n",
    "    # extracting Title\n",
    "    titleBoard = soup.find('span', id = 'productTitle')\n",
    "    title = titleBoard.text\n",
    "\n",
    "    # extracting ratings\n",
    "    ratingBoard = soup.find('div', class_ = 'averageCustomerReviews')\n",
    "    # extracting average rating\n",
    "    avgRatingBoard = ratingBoard.find('span', class_ = 'a-size-base a-color-base')\n",
    "    avgRating = avgRatingBoard.text.strip()\n",
    "    # extracting rating number\n",
    "    ratingNoBoard = ratingBoard.find('span', id = 'acrCustomerReviewText')\n",
    "    ratingNo = ratingNoBoard.text.strip()\n",
    "\n",
    "    # extracting price\n",
    "    priceBoard = soup.find('div', class_ = 'a-section a-spacing-none aok-align-center aok-relative').find('span', class_ = 'aok-offscreen')\n",
    "    price = priceBoard.text.strip()\n",
    "\n",
    "    # exracting media\n",
    "    mediaBoard = soup.find('ul', class_ = 'a-unordered-list a-nostyle a-button-list a-vertical a-spacing-top-micro regularAltImageViewLayout')\n",
    "    media = mediaBoard.find_all('li')\n",
    "\n",
    "    # extracting the store\n",
    "    storeBoard = soup.find('tr', class_ = 'a-spacing-small po-brand').find('td', class_ = 'a-span9')\n",
    "    store = storeBoard.text.strip()\n",
    "\n",
    "    # extracting features\n",
    "    featuresBoard = soup.find('ul', class_ = 'a-unordered-list a-vertical a-spacing-mini').find_all('il')\n",
    "    features = featuresBoard\n",
    "\n",
    "    # extracting details\n",
    "    detailsBoard = soup.find('div', class_ = 'a-expander-content a-expander-section-content a-section-expander-inner').find('table', class_ = 'a-keyvalue prodDetTable').find_all('tr')\n",
    "    details = detailsBoard\n",
    "\n",
    "    detailsBoardB = soup.find('div', id = 'detailBullets_feature_div').find_all('li')\n",
    "    detailsB = detailsBoardB\n",
    "\n",
    "    detailsBoardC = soup.find('table', id = 'productDetails_detailBullets_sections1').find_all('tr')\n",
    "\n",
    "    asinBorad = soup.find('tr', text=lambda x: x.strip() == 'ASIN', class_='prodDetSectionEntry')\n",
    "    asin = asinBoard.text.strip();\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the date strings\n",
    "date_str1 = \"June 22, 2016\"\n",
    "date_str2 = \"September 30, 2023\"\n",
    "\n",
    "# Convert the date strings to datetime objects\n",
    "date1 = datetime.strptime(date_str1, \"%B %d, %Y\")\n",
    "date2 = datetime.strptime(date_str2, \"%B %d, %Y\")\n",
    "\n",
    "# Compare the dates\n",
    "if date1 > date2:\n",
    "    print(f\"{date_str1} is greater than {date_str2}\")\n",
    "else:\n",
    "    print(f\"{date_str1} is not greater than {date_str2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
