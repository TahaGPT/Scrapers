{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup as bsp\n",
    "from selenium import webdriver\n",
    "from seleniumwire import webdriver  # blinker == 1.7.0\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.edge.service import Service as EdgeService\n",
    "from selenium.webdriver.edge.options import Options as EdgeOptions\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from fake_useragent import UserAgent\n",
    "# from googlesearch import search\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "base = 'https://www.amazon.com'\n",
    "thirdWheel = '/s?k='\n",
    "date_pattern = r\"\\b(January|February|March|April|May|June|July|August|September|October|November|December) \\d{1,2}, \\d{4}\\b\"\n",
    "number_pattern = r\"\\d+\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be Run Yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file, skipping the header\n",
    "file_path = \"AmazonCategory.csv\"\n",
    "df = pd.read_csv(file_path, delimiter=\";\", header=None, skiprows=1)\n",
    "\n",
    "# Create the dictionary with subcategories as keys and categories as values\n",
    "category = {}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    dawcategory, subcategory = row[0], row[1]\n",
    "    \n",
    "    if pd.notna(subcategory):  # Ignore empty subcategories\n",
    "        category[subcategory] = dawcategory\n",
    "\n",
    "# Print the dictionary\n",
    "for subcategory, dawcategory in category.items():\n",
    "    print(f\"Subcategory: {subcategory} â†’ Category: {dawcategory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session = requests.Session()\n",
    "# session.headers.update({\"User-Agent\": UserAgent().random})\n",
    "driver = webdriver.Chrome()\n",
    "# driver = webdriver.Firefox(service = service, options = options)\n",
    "driver.get(base)\n",
    "#   It will scroll to right above the footer of the page then scoll to the top then back to the bottom untill there is no new items being loaded\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(10) \n",
    "html = driver.page_source\n",
    "driver.quit()          \n",
    "soup = bsp(html, 'lxml')\n",
    "boxes = soup.find_all('a', class_ = 'a-link-normal _fluid-quad-image-label-v2_style_centerImage__30wh- aok-block image-window')\n",
    "print(boxes)\n",
    "links = []\n",
    "urls = []\n",
    "categ = []\n",
    "for box in boxes:\n",
    "    actual = box.get('href')\n",
    "    url = base + actual\n",
    "    print(\"Url : \", url)\n",
    "    urls.append(url)\n",
    "    som = box.get('aria-label')\n",
    "    link = som.replace(' ', '+')\n",
    "    link = base + thirdWheel + link\n",
    "    print(\"Links : \", link)\n",
    "    links.append(link)\n",
    "    categ.append(som)\n",
    "    print(som)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = {}\n",
    "for cat in category:\n",
    "    url = base+thirdWheel+cat\n",
    "    urls[url] = category[cat]\n",
    "print(urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_proxies():\n",
    "    proxy_list_url = 'https://free-proxy-list.net/'\n",
    "    response = requests.get(proxy_list_url)\n",
    "    soup = bsp(response.text, 'html.parser')\n",
    "    proxy_data = []\n",
    "    rows = soup.find_all('tr')[1:]\n",
    "    for row in rows:\n",
    "        columns = row.find_all('td')\n",
    "        if len(columns) >= 8:\n",
    "            ip_address = columns[0].text.strip()\n",
    "            google_enabled = columns[5].text.strip().lower() == 'yes'\n",
    "            https_enabled = columns[6].text.strip().lower() == 'yes'\n",
    "            last_checked = columns[7].text.strip()\n",
    "            if (last_checked.endswith('mins ago') and int(last_checked.split(' ')[0]) < 15) or last_checked.endswith('hours ago'):\n",
    "                if google_enabled or https_enabled:\n",
    "                    proxy_data.append({'ip_address': ip_address, 'google_enabled': google_enabled, 'https_enabled': https_enabled})\n",
    "\n",
    "    return proxy_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_user_agent(proxy):\n",
    "    if proxy:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "            'http': f'http://{proxy}',\n",
    "            'https': f'https://{proxy}'\n",
    "        }\n",
    "    else:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "        }\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = []\n",
    "# categories = []\n",
    "cat = 0\n",
    "me = False\n",
    "# proxies = get_valid_proxies()\n",
    "\n",
    "for url, cat in urls.items():\n",
    "    total_samples = 0\n",
    "    myurl = url\n",
    "    while myurl:\n",
    "        # proxy = proxies[total_samples % len(proxies)] if proxies else None\n",
    "        options = {\n",
    "            'headers' : {\n",
    "                \"User-Agent\": UserAgent().random\n",
    "            }\n",
    "        }\n",
    "        print(\"VISITING : \", myurl)\n",
    "        # sdsd = Options()\n",
    "        # sdsd.add_argument(\"--headless\")  # Run in background\n",
    "        # sdsd.add_argument(\"--disable-gpu\")\n",
    "        # sdsd.add_argument(\"--no-sandbox\")\n",
    "        # sdsd.add_argument(\"--disable-dev-shm-usage\")\n",
    "        driver = webdriver.Chrome(seleniumwire_options = options)\n",
    "        # driver = webdriver.Firefox()\n",
    "        driver.get(myurl)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "        time.sleep(10)\n",
    "        html = driver.page_source\n",
    "        driver.quit()\n",
    "        # headers = rotate_user_agent(proxy)\n",
    "        # response = req.get(myurl, headers = headers)\n",
    "        # session = requests.Session()\n",
    "        # session.headers.update({\"User-Agent\": UserAgent().random})\n",
    "        # response = req.get(myurl, headers = {\"User-Agent\": UserAgent().random})\n",
    "        # if response.status_code == 200:\n",
    "            # soup = bsp(response.text, 'lxml')\n",
    "        soup = bsp(html, 'lxml')\n",
    "        # print(soup)\n",
    "        productes = soup.find_all('a', class_ = 'a-link-normal s-no-outline')\n",
    "        if not productes:\n",
    "            productes = soup.find_all('a', class_ = 'a-link-normal s-line-clamp-4 s-link-style a-text-normal')\n",
    "        if not productes:\n",
    "            productes = soup.find_all('a' , class_= 'a-link-normal s-line-clamp-2 s-link-style a-text-normal')\n",
    "        print(productes)\n",
    "        for product in productes:\n",
    "            product = base + product.get('href')\n",
    "            products.append(product)\n",
    "            print(product)\n",
    "        csv_file = 'Amazon.csv'\n",
    "        cat = cat.replace('+', \" \")\n",
    "        info = {\"Products\": products, \"Categories\": cat}\n",
    "        new_df = pd.DataFrame(info)\n",
    "        if not me:\n",
    "            new_df.to_csv(csv_file, mode='a', index = False)\n",
    "            me = True\n",
    "        else:\n",
    "            new_df.to_csv(csv_file, mode='a', header=False, index = False)\n",
    "        print(\"|||||||||||||||||||||||||||| Data appended successfully with continued index. |||||||||||||||||||||||||||\")\n",
    "        \n",
    "        total_samples += 1\n",
    "        # Reset lists to prevent duplicates in the next iteration\n",
    "        products.clear()\n",
    "        nextP = soup.find('a', class_ = 's-pagination-item s-pagination-next s-pagination-button s-pagination-button-accessibility s-pagination-separator')\n",
    "        if nextP:\n",
    "            myurl = base + nextP.get('href')\n",
    "        else:\n",
    "            break\n",
    "    cat += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csv_file = 'Amazon.csv'\n",
    "meta = 'Products.csv'\n",
    "rev = 'Reviews.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "samples = 0\n",
    "me = False\n",
    "for _,row in df.iterrows():\n",
    "    agent = {\n",
    "        \"link\" : \"\",\n",
    "        \"date\" : \"\",\n",
    "        \"main_category\" : \"\",\n",
    "        \"title\" : \"\",\n",
    "        \"average_rating\" : 0.0,\n",
    "        \"rating_number\" : 0,\n",
    "        \"features\":[],\n",
    "        \"description\":[],\n",
    "        \"price\" : \"\",\n",
    "        \"images\":[],\n",
    "        \"videos\":[],\n",
    "        \"store\" : \"\",\n",
    "        \"categories\" : \"\",\n",
    "        \"details\":{},\n",
    "        \"parent_asin\" : \"\",\n",
    "        \"bought_together\": []\n",
    "    }\n",
    "\n",
    "    comment = {\n",
    "        \"link\" : \"\",\n",
    "        \"rating\" : 0.0,\n",
    "        \"title\" : \"\",\n",
    "        \"text\" : \"\",\n",
    "        \"images\" : \"\",\n",
    "        \"asin\" : \"\",\n",
    "        \"parent_asin\" : \"\",\n",
    "        \"user_id\" : \"\",\n",
    "        \"timestamp\" : \"\",\n",
    "        \"verified_purchase\" : 0,\n",
    "        \"helpful_vote\" : \"\"\n",
    "    }\n",
    "    print(\"\\n\\n\\n\\n\\n\\n-------------------------------------------------------------------Product-------------------------------------------------------------------\")\n",
    "    options = {\n",
    "            'headers' :{\n",
    "                \"User-Agent\":UserAgent().random\n",
    "            }\n",
    "        }\n",
    "    link = row['Products']\n",
    "    print(link)\n",
    "    driver = webdriver.Chrome(seleniumwire_options = options)\n",
    "    driver.get(link)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "    time.sleep(10)\n",
    "    html = driver.page_source\n",
    "    driver.quit()\n",
    "    soup = bsp(html, 'lxml')\n",
    "\n",
    "    feat = []\n",
    "    img = []\n",
    "    vid = []\n",
    "    det = []\n",
    "\n",
    "    agent[\"link\"] = link\n",
    "\n",
    "\n",
    "    # extracting Category\n",
    "    category = row['Categories']\n",
    "    print(\"Category :\", category)\n",
    "    agent[\"main_category\"] = category\n",
    "\n",
    "    # extracting Title\n",
    "    try:\n",
    "        titleBoard = soup.find('span', id = 'productTitle')\n",
    "        title = titleBoard.text.strip()\n",
    "    except:\n",
    "        title = \"\"\n",
    "    print(\"Title :\", title)\n",
    "    agent[\"title\"] = title\n",
    "\n",
    "    # extracting ratings\n",
    "    try:\n",
    "        ratingBoard = soup.find('div', id = 'averageCustomerReviews')\n",
    "        # extracting average rating\n",
    "        avgRatingBoard = ratingBoard.find('span', class_ = 'a-size-base a-color-base')\n",
    "        avgRating = avgRatingBoard.text.strip()\n",
    "        # extracting rating number\n",
    "        ratingNoBoard = ratingBoard.find('span', id = 'acrCustomerReviewText')\n",
    "        ratingNo = ratingNoBoard.text.strip()\n",
    "        junk = \" ratings\"\n",
    "        ratingNo = ratingNo.replace(junk, \"\")\n",
    "    except:\n",
    "        avgRating = \"\"\n",
    "        ratingNo = \"\"\n",
    "    print(\"Average Rating :\", avgRating)\n",
    "    agent[\"average_rating\"] = avgRating\n",
    "    print(\"Rating :\", ratingNo)\n",
    "    agent[\"rating_number\"] = ratingNo\n",
    "\n",
    "    # extracting price\n",
    "    try:\n",
    "        priceBoard = soup.find('div', class_ = 'a-section a-spacing-none aok-align-center aok-relative').find('span', class_ = 'aok-offscreen')\n",
    "        price = priceBoard.text.strip()[0:5]\n",
    "    except:\n",
    "        price = \"\"\n",
    "    print(\"Price :\", price)\n",
    "    agent[\"price\"] = price\n",
    "\n",
    "    # exracting media\n",
    "    try:\n",
    "        mediaBoard = soup.find('ul', class_ = 'a-unordered-list a-nostyle a-button-list a-vertical a-spacing-top-micro regularAltImageViewLayout')\n",
    "        if not mediaBoard:\n",
    "            mediaBoard = soup.find('ul', class_ = 'a-unordered-list a-nostyle a-button-list a-vertical a-spacing-top-micro gridAltImageViewLayoutIn1x7')\n",
    "            if not mediaBoard:\n",
    "                mediaBoard = soup.find('ul', class_ = 'a-unordered-list a-nostyle a-button-list a-vertical a-spacing-top-extra-large regularAltImageViewLayout')\n",
    "        media = mediaBoard.find_all('img')\n",
    "        images = [img['src'] for img in media]\n",
    "        vids = images[-1]\n",
    "        images = images[0:-1]\n",
    "    except:\n",
    "        images = []\n",
    "        vids = []\n",
    "    print(\"Images :\", images)\n",
    "    agent[\"images\"] = images   \n",
    "    print(\"Videos :\", vids)\n",
    "    agent[\"videos\"] = vids\n",
    "\n",
    "    # extracting the store\n",
    "    try:\n",
    "        storeBoard = soup.find('tr', class_ = 'a-spacing-small po-brand').find('td', class_ = 'a-span9')\n",
    "        store = storeBoard.text.strip()\n",
    "    except:\n",
    "        try:\n",
    "            storeBoard = soup.find('a', id = 'bylineInfo').text.strip()\n",
    "            junk = \"Visit the \"\n",
    "            store = storeBoard.replace(junk, \"\")\n",
    "        except:\n",
    "            store = \"\"\n",
    "    print(\"Store :\", store)\n",
    "    agent[\"store\"] = store\n",
    "\n",
    "    # extracting features\n",
    "    try:\n",
    "        featuresBoard = soup.find('ul', class_ = 'a-unordered-list a-vertical a-spacing-mini').find_all('li')\n",
    "        features = [feature.text.strip() for feature in featuresBoard]\n",
    "    except:\n",
    "        features = []\n",
    "    print(\"Features :\", features)\n",
    "    agent[\"features\"] = features\n",
    "        \n",
    "    # extracting details\n",
    "    try:\n",
    "        detailsBoard = soup.find('div', class_ = 'a-expander-content a-expander-section-content a-section-expander-inner').find('table', class_ = 'a-keyvalue prodDetTable').find_all('tr')\n",
    "    except:\n",
    "        try:\n",
    "            detailsBoard = soup.find('div', id = 'detailBullets_feature_div').find_all('li')\n",
    "        except:\n",
    "            try:\n",
    "                detailsBoard = soup.find('table', id = 'productDetails_detailBullets_sections1').find_all('tr')\n",
    "            except:   \n",
    "                try:\n",
    "                    detailsBoard = soup.find('ul', class_ = 'a-unordered-list a-nostyle a-vertical a-spacing-none detail-bullet-list').find_all('tr')\n",
    "                except:\n",
    "                    detailsBoard = []\n",
    "\n",
    "    # find th for the dictionary subject and td for the disctionary details\n",
    "    details = {}\n",
    "    if detailsBoard:\n",
    "        for detailBoard in detailsBoard:\n",
    "            detailingH = detailBoard.find('th').text.strip() if detailBoard.find('th') else None\n",
    "            detailingA = detailBoard.find('td').text.strip() if detailBoard.find('td') else None\n",
    "            if detailingH and detailingA:\n",
    "                detailed = {detailingH: detailingA} \n",
    "                details |= detailed  \n",
    "    else:\n",
    "        details = {}\n",
    "    print(\"Details :\", details)\n",
    "    agent[\"details\"] = details\n",
    "                    \n",
    "    # extracting asin\n",
    "    try:\n",
    "        asin = details['ASIN']\n",
    "    except:\n",
    "        asin = \"\"\n",
    "    print(\"ASIN :\", asin)\n",
    "    agent[\"parent_asin\"] = asin\n",
    "\n",
    "    try:\n",
    "        date = details['Date First Available']\n",
    "    except:\n",
    "        date = \"\"\n",
    "    print(\"Date :\", date)\n",
    "    agent[\"date\"] = date\n",
    "    \n",
    "    # extracting description\n",
    "    try:\n",
    "        descriptionBoard = soup.find('table', class_ = 'a-normal a-spacing-micro')\n",
    "        if not descriptionBoard:\n",
    "            descriptionBoard = soup.find('ul', id = 'a-nostyle').find_all('div', class_ = 'a-fixed-left-grid-col a-col-right')\n",
    "        descriptions = descriptionBoard.find_all('td', class_ = 'a-span9')\n",
    "        description = [dec.text.strip() for dec in descriptions]\n",
    "    except:\n",
    "        description = []\n",
    "    print(\"Description :\", description)\n",
    "    agent[\"description\"] = description\n",
    "\n",
    "    # extracting bought together\n",
    "    try:\n",
    "        boughtTogetherBoard = soup.find('div', id = 'a-cardui _c3AtZ_new-thumbnail-box_1W9Ku _c3AtZ_two-item-thumbnail-box_7kF95')\n",
    "        if not boughtTogetherBoard:\n",
    "            boughtTogetherBoard = soup.find('div', class_ = 'a-cardui _p13n-desktop-sims-fbt_fbt-desktop_new-thumbnail-box__36bD3 _p13n-desktop-sims-fbt_fbt-desktop_two-item-thumbnail-box__jV2am')\n",
    "        if not boughtTogetherBoard:\n",
    "            boughtTogetherBoard = soup.find('div', class_ = 'a-cardui _p13n-desktop-sims-fbt_fbt-desktop_new-thumbnail-box__36bD3')\n",
    "        boughtTogetherS = boughtTogetherBoard.find_all('a', class_ = 'a-link-normal a-text-normal', href = True)\n",
    "        if not boughtTogetherS:\n",
    "            boughtTogetherS = boughtTogetherBoard.find_all('a', class_ = 'a-link-normal', href = True)        \n",
    "        boughtTogether = [base + link['href'] for link in boughtTogetherS]\n",
    "    except:\n",
    "        boughtTogether = []\n",
    "    print(\"Bought Together :\", boughtTogether)\n",
    "    agent[\"bought_together\"] = boughtTogether\n",
    "\n",
    "    # extracting further categories\n",
    "    try:\n",
    "        categoriesBoard = soup.find_all('div', id= 'tp-inline-twister-dim-values-container')\n",
    "        categoriesBoardAh = categoriesBoard[-1].find_all('li')\n",
    "        categories = [categoriesBoardh.text.strip() for categoriesBoardh in categoriesBoardAh]\n",
    "    except:\n",
    "        categories = []\n",
    "    print(\"Categories :\", categories)\n",
    "    agent[\"categories\"] = categories\n",
    "\n",
    "\n",
    "    ############################################################################################################################################################\n",
    "    print(\"\\n\\n\\n-------------------------------------------------------------------Comments-------------------------------------------------------------------\")\n",
    "    # extracting reviews\n",
    "    reviewsUS = soup.find('ul', id = 'cm-cr-dp-review-list').find_all('li')\n",
    "    reviewsGlobal = soup.find('ul', id = 'cm-cr-global-review-list').find_all('li')\n",
    "    reviews = reviewsUS + reviewsGlobal\n",
    "    for review in reviews:\n",
    "        # extracting rating\n",
    "        try:\n",
    "            reviewRatingBoard = review.find('i', {'data-hook': 'review-star-rating'}).find('span', class_ = 'a-icon-alt')\n",
    "            reviewRating = reviewRatingBoard.text[0:3]\n",
    "        except:\n",
    "            reviewRating = \"\"\n",
    "        print(\"Rating :\", reviewRating)\n",
    "        comment[\"rating\"].append(reviewRating)\n",
    "\n",
    "        # extracting title\n",
    "        try:\n",
    "            reviewTitleBoard = review.find('a', class_ = 'a-size-base a-link-normal review-title a-color-base review-title-content a-text-bold')\n",
    "            reviewTitles = reviewTitleBoard.find_all('span')\n",
    "            reviewTitle = reviewTitles[-1].text.strip()\n",
    "        except:\n",
    "            reviewTitle = \"\"\n",
    "        print(\"Title :\", reviewTitle)\n",
    "        comment[\"title\"].append(reviewTitle)\n",
    "\n",
    "        # extracting Text\n",
    "        try:\n",
    "            reviewTextBoard = review.find('div', class_ = 'a-expander-content reviewText review-text-content a-expander-partial-collapse-content')\n",
    "            reviewText = reviewTextBoard.text.strip()\n",
    "        except:\n",
    "            reviewText = \"\"\n",
    "        print(\"Review :\", reviewText)\n",
    "        comment[\"text\"].append(reviewText)\n",
    "\n",
    "        # extracting images\n",
    "        try:\n",
    "            reviewImages = review.find('div', class_ = 'review-image-tile-section').find_all('img')\n",
    "            Images = [img['src'] for img in reviewImages]\n",
    "        except:\n",
    "            Images = []\n",
    "        print(\"Images :\", Images)\n",
    "        comment[\"images\"].append(Images)\n",
    "\n",
    "        # extracting ASIN\n",
    "        reviewAsin = review.get('id')\n",
    "        print(\"ASIN :\", reviewAsin)\n",
    "        comment[\"asin\"].append(reviewAsin)\n",
    "\n",
    "        # extracting parent ASIN\n",
    "        reviewParentAsin = asin\n",
    "        print(\"Parent ASIN :\", reviewParentAsin)\n",
    "        comment[\"parent_asin\"].append(review)\n",
    "\n",
    "        # eaxtracting username\n",
    "        try:\n",
    "            username = review.find('span', class_ = 'a-profile-name').text.strip()\n",
    "        except:\n",
    "            username = \"\"\n",
    "        print(\"Username :\", username)\n",
    "        comment[\"user_id\"].append(username)\n",
    "\n",
    "        # extracting date\n",
    "        try:\n",
    "            reviewTimeBoard = review.find('span', class_ = 'a-size-base a-color-secondary review-date')\n",
    "            reviewTimes = reviewTimeBoard.text\n",
    "            reviewTimes = re.search(date_pattern, reviewTimes)\n",
    "            reviewTiming = datetime.strptime(reviewTimes, \"%B %d, %Y\")\n",
    "            reviewTime = int(reviewTiming.timestamp())\n",
    "        except:\n",
    "            reviewTime = \"\"\n",
    "        print(\"Date :\", reviewTime)\n",
    "        comment[\"timestamp\"].append(reviewTime)\n",
    "\n",
    "        verified = False\n",
    "        if review.find('span', class_ = 'a-size-mini a-color-state a-text-bold'):\n",
    "            verified = True\n",
    "        print(\"Verified :\", verified)\n",
    "        comment[\"verified_purchase\"].append(verified)\n",
    "\n",
    "        # extracting helpfulness\n",
    "        try:\n",
    "            reviewHelpful = review.find('span', class_ = 'a-size-base a-color-tertiary cr-vote-text')\n",
    "            reviewHelpful = reviewHelpful.text\n",
    "            haha = \"One\"\n",
    "            helpful = 0\n",
    "            Helpfulness = re.search(number_pattern, reviewHelpful)\n",
    "            if Helpfulness:\n",
    "                helpful = int(Helpfulness.group())\n",
    "            elif haha in reviewHelpful:\n",
    "                helpful = 1\n",
    "        except:\n",
    "            helpful = 0\n",
    "        print(\"Helpful :\", helpful)\n",
    "        comment[\"helpful_vote\"].append(helpful)\n",
    "\n",
    "        myMeta = pd.DataFrame(agent)\n",
    "        myRev = pd.DataFrame(comment)\n",
    "    if not me:\n",
    "        myMeta.to_csv(meta, mode='a', index = False)\n",
    "        myRev.to_csv(rev, mode='a', index = False)\n",
    "        me = True\n",
    "    else:\n",
    "        myMeta.to_csv(meta, mode='a', header=False, index = False)\n",
    "        myRev.to_csv(rev, mode='a', header=False, index = False)\n",
    "    \n",
    "    print(\"|||||||||||||||||||||||||||| Data appended successfully with continued index. |||||||||||||||||||||||||||\")\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the date strings\n",
    "date_str1 = \"June 22, 2016\"\n",
    "date_str2 = \"September 30, 2023\"\n",
    "\n",
    "# Convert the date strings to datetime objects\n",
    "date1 = datetime.strptime(date_str1, \"%B %d, %Y\")\n",
    "date2 = datetime.strptime(date_str2, \"%B %d, %Y\")\n",
    "\n",
    "# Compare the dates\n",
    "if date1 > date2:\n",
    "    print(f\"{date_str1} is greater than {date_str2}\")\n",
    "else:\n",
    "    print(f\"{date_str1} is not greater than {date_str2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date string in the format \"Month Day, Year\"\n",
    "date_str = \"January 17, 2025\"\n",
    "\n",
    "# Parse the string into a datetime object\n",
    "date_obj = datetime.strptime(date_str, \"%B %d, %Y\")\n",
    "\n",
    "# Convert to Unix time\n",
    "unix_time = int(date_obj.timestamp())\n",
    "\n",
    "print(f\"Unix Time: {unix_time}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example statement\n",
    "statement = \"The item costs 700 dollars, and the discount is.\"\n",
    "\n",
    "# Regular expression to match one or more digits\n",
    "number_pattern = r\"\\d+\"\n",
    "\n",
    "# Find the first number in the statement\n",
    "match = re.search(number_pattern, statement)\n",
    "\n",
    "if match:\n",
    "    number = int(match.group())  # Convert the matched number to an integer\n",
    "    print(f\"Extracted number: {number}\")\n",
    "else:\n",
    "    print(\"No number found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
