{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup as bsp\n",
    "from selenium import webdriver\n",
    "from seleniumwire import webdriver  # blinker == 1.7.0\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.edge.service import Service as EdgeService\n",
    "from selenium.webdriver.edge.options import Options as EdgeOptions\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from fake_useragent import UserAgent\n",
    "# from googlesearch import search\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "basis = 'https://www.amazon.com/gp/new-releases'\n",
    "base = 'https://www.amazon.com'\n",
    "thirdWheel = '/s?k='\n",
    "date_pattern = r\"\\b(January|February|March|April|May|June|July|August|September|October|November|December) \\d{1,2}, \\d{4}\\b\"\n",
    "number_pattern = r\"\\d+\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Releases Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = {}\n",
    "\n",
    "options = {\n",
    "    'headers' : {\n",
    "        \"User-Agent\": UserAgent().random\n",
    "    }\n",
    "}\n",
    "print(\"VISITING : \", basis)\n",
    "driver = webdriver.Chrome(seleniumwire_options = options)\n",
    "# driver = webdriver.Firefox()\n",
    "driver.get(basis)\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "time.sleep(10)\n",
    "html = driver.page_source\n",
    "driver.quit()\n",
    "soup = bsp(html, 'lxml')\n",
    "board = soup.find('div', class_ = '_p13n-zg-nav-tree-all_style_zg-browse-group__88fbz')\n",
    "moi = board.find_all('a')\n",
    "for haha in moi:\n",
    "    try:\n",
    "        cats[haha.text] = base + haha['href']\n",
    "    except:\n",
    "        pass\n",
    "print(\"CATEGORIES : \", cats)\n",
    "df = pd.DataFrame(list(cats.items()), columns=['Category', 'Link'])\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"NewCategory.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting More from New Releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _p13n-zg-nav-tree-all_style_zg-browse-group__88fbz\n",
    "df = pd.read_csv(\"NewCategory.csv\")\n",
    "\n",
    "    # Extract the link from the second column and add the array to the third column\n",
    "for _, row in df.iterrows():\n",
    "    link = row[1]  # Assuming the second column contains the link\n",
    "    options = {\n",
    "    'headers' : {\n",
    "        \"User-Agent\": UserAgent().random\n",
    "        }\n",
    "    }\n",
    "    print(\"VISITING : \", link)\n",
    "    driver = webdriver.Chrome(seleniumwire_options = options)\n",
    "    # driver = webdriver.Firefox()\n",
    "    driver.get(link)\n",
    "    time.sleep(10)\n",
    "\n",
    "    # while True:\n",
    "    #     # It will scroll to right above the footer of the page then scoll to the top then back to the bottom untill there is no new items being loaded\n",
    "    #     driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    #     time.sleep(5)\n",
    "    #     driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "    #     time.sleep(5)\n",
    "    #     driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    #     time.sleep(5)\n",
    "    #     New_Height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    #     if New_Height == Previous_Height:\n",
    "    #         break\n",
    "    #     Previous_Height = New_Height\n",
    "    #     # if there is a popup press the close button\n",
    "    #     try:\n",
    "    #         driver.find_element_by_class_name('modal-close').click()\n",
    "    #     except:\n",
    "    #         pass\n",
    "\n",
    "    html = driver.page_source\n",
    "    driver.quit()\n",
    "    soup = bsp(html, 'lxml')\n",
    "    board = soup.find('div', class_ = '_p13n-zg-nav-tree-all_style_zg-browse-group__88fbz', role = 'group')\n",
    "    linkers = board.find_all('a')\n",
    "    links = []\n",
    "    for haha in linkers:\n",
    "        try:\n",
    "            links.append(base + haha['href'])\n",
    "        except:\n",
    "            pass\n",
    "    print(\"LINKS : \", links)\n",
    "    # Add the links to the third column of the DataFrame\n",
    "    df.at[_, 'Links'] = ', '.join(links)\n",
    "df.to_csv(\"NewCategory.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ML\\AppData\\Local\\Temp\\ipykernel_17576\\3140413388.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  category = row[0]  # Assuming the first column contains the category name\n",
      "C:\\Users\\ML\\AppData\\Local\\Temp\\ipykernel_17576\\3140413388.py:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  productLinks = row[2]  # Assuming the second column contains the link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VISITING :  https://www.amazon.com/gp/new-releases/amazon-devices/370783011/ref=zg_bsnr_nav_amazon-devices_1\n",
      "LINKS :  ['https://www.amazon.com/Amazon-Paperwhite-Lightweight-Water-Safe-Protective/dp/B0CM7X424T/ref=zg_bsnr_g_370783011_d_sccl_1/132-6980086-9459402?psc=1', 'https://www.amazon.com/Ring-Rechargeable-Quick-Release-Battery/dp/B0F1TQBK9H/ref=zg_bsnr_g_370783011_d_sccl_2/132-6980086-9459402?psc=1', 'https://www.amazon.com/2-Year-Protection-Kindle-Paper-White/dp/B0C8S1Q6G7/ref=zg_bsnr_g_370783011_d_sccl_3/132-6980086-9459402?psc=1', 'https://www.amazon.com/Amazon-Paperwhite-Lightweight-Water-Safe-Protective/dp/B0CM7RF1KR/ref=zg_bsnr_g_370783011_d_sccl_4/132-6980086-9459402?psc=1', 'https://www.amazon.com/All-new-Blink-Sync-Module-XR/dp/B0B198XD6X/ref=zg_bsnr_g_370783011_d_sccl_5/132-6980086-9459402?psc=1', 'https://www.amazon.com/Amazon-Kindle-Case/dp/B0CX4XSY3X/ref=zg_bsnr_g_370783011_d_sccl_6/132-6980086-9459402?psc=1', 'https://www.amazon.com/Blink-Sync-Module-XR-2Cam/dp/B0D9HVYZR4/ref=zg_bsnr_g_370783011_d_sccl_7/132-6980086-9459402?psc=1', 'https://www.amazon.com/Blink-Sync-Module-XR-1Cam/dp/B0D9HWGFK7/ref=zg_bsnr_g_370783011_d_sccl_8/132-6980086-9459402?psc=1', 'https://www.amazon.com/Amazon-Paperwhite-Lightweight-Water-Safe-Protective/dp/B0CM7PHKMJ/ref=zg_bsnr_g_370783011_d_sccl_9/132-6980086-9459402?psc=1', 'https://www.amazon.com/Introducing-Amazon-Plant-Based-Lightweight-Sustainability/dp/B0CM7ZXQWC/ref=zg_bsnr_g_370783011_d_sccl_10/132-6980086-9459402?psc=1', 'https://www.amazon.com/Introducing-Amazon-Plant-Based-Lightweight-Sustainability/dp/B0CM82H71F/ref=zg_bsnr_g_370783011_d_sccl_11/132-6980086-9459402?psc=1', 'https://www.amazon.com/2-Year-Protection-Plan-for-Kindle/dp/B0CNY22DJP/ref=zg_bsnr_g_370783011_d_sccl_12/132-6980086-9459402?psc=1', 'https://www.amazon.com/2-Year-Accident-Protection-Kindle-Scribe/dp/B0CZB1F8HL/ref=zg_bsnr_g_370783011_d_sccl_13/132-6980086-9459402?psc=1', 'https://www.amazon.com/Antiglare-Protector-Paperwhite-Releases-Colorsoft/dp/B0DPNBNJVS/ref=zg_bsnr_g_370783011_d_sccl_14/132-6980086-9459402?psc=1', 'https://www.amazon.com/Amazon-Paperwhite-Colorsoft-Lightweight-Protective/dp/B0CM7MZ99X/ref=zg_bsnr_g_370783011_d_sccl_15/132-6980086-9459402?psc=1', 'https://www.amazon.com/Introducing-Amazon-Plant-Based-Lightweight-Sustainability/dp/B0CM79Z8TG/ref=zg_bsnr_g_370783011_d_sccl_16/132-6980086-9459402?psc=1', 'https://www.amazon.com/2-Year-Protection-Amazon-Kindle-Colorsoft/dp/B0DG11DGXC/ref=zg_bsnr_g_370783011_d_sccl_17/132-6980086-9459402?psc=1', 'https://www.amazon.com/echo-show-15-stand/dp/B0DK2XLYKR/ref=zg_bsnr_g_370783011_d_sccl_18/132-6980086-9459402?psc=1', 'https://www.amazon.com/Introducing-Plant-Based-Magnetic-Sustainability-Amazon/dp/B0D5KR4JQ3/ref=zg_bsnr_g_370783011_d_sccl_19/132-6980086-9459402?psc=1', 'https://www.amazon.com/Blink-MicroSD-card/dp/B0D715PVDQ/ref=zg_bsnr_g_370783011_d_sccl_20/132-6980086-9459402?psc=1', 'https://www.amazon.com/Introducing-Plant-Based-Magnetic-Sustainability-Amazon/dp/B0D5KKYKLZ/ref=zg_bsnr_g_370783011_d_sccl_21/132-6980086-9459402?psc=1', 'https://www.amazon.com/Amazon-Paperwhite-Colorsoft-Lightweight-Protective/dp/B0CM7JC6CN/ref=zg_bsnr_g_370783011_d_sccl_22/132-6980086-9459402?psc=1', 'https://www.amazon.com/2-Year-Protection-Plan-Echo-Show/dp/B0C5F4GJJW/ref=zg_bsnr_g_370783011_d_sccl_23/132-6980086-9459402?psc=1', 'https://www.amazon.com/All-New-Screen-Protector-Amazon-Release/dp/B0DKV8YKCH/ref=zg_bsnr_g_370783011_d_sccl_24/132-6980086-9459402?psc=1', 'https://www.amazon.com/All-New-Ring-Stake-Cameras-Panels/dp/B0B833L2F3/ref=zg_bsnr_g_370783011_d_sccl_25/132-6980086-9459402?psc=1', 'https://www.amazon.com/All-New-Spider-Man-Amazon-Generations-Separately/dp/B0D5NQJM3S/ref=zg_bsnr_g_370783011_d_sccl_26/132-6980086-9459402?psc=1', 'https://www.amazon.com/All-New-Ring-Stake-Cameras-Panels/dp/B0B8328Z43/ref=zg_bsnr_g_370783011_d_sccl_27/132-6980086-9459402?psc=1', 'https://www.amazon.com/echo-show-21-stand/dp/B0DK2ZNW5B/ref=zg_bsnr_g_370783011_d_sccl_28/132-6980086-9459402?psc=1', 'https://www.amazon.com/All-New-Ring-Mount-Cameras-Panels/dp/B0CG2KKPXD/ref=zg_bsnr_g_370783011_d_sccl_29/132-6980086-9459402?psc=1', 'https://www.amazon.com/All-New-Made-Amazon-Frame-Release/dp/B0CQ1WXJ8P/ref=zg_bsnr_g_370783011_d_sccl_30/132-6980086-9459402?psc=1', 'https://www.amazon.com/2-Year-Protection-Plan-Echo-Show/dp/B0CF954BG8/ref=zg_bsnr_g_370783011_d_sccl_31/132-6980086-9459402?psc=1', 'https://www.amazon.com/Amazon-Kindle-Scribe-Magnetic-Protective/dp/B0D5KPDR1X/ref=zg_bsnr_g_370783011_d_sccl_32/132-6980086-9459402?psc=1', 'https://www.amazon.com/All-New-Made-Amazon-Frame-Release/dp/B0CQ21298C/ref=zg_bsnr_g_370783011_d_sccl_33/132-6980086-9459402?psc=1', 'https://www.amazon.com/Made-Amazon-Remote-Holder-Remotes/dp/B0D852LMHN/ref=zg_bsnr_g_370783011_d_sccl_34/132-6980086-9459402?psc=1', 'https://www.amazon.com/All-New-Amazon-Standing-Remote-Release/dp/B0D4PMRRXV/ref=zg_bsnr_g_370783011_d_sccl_35/132-6980086-9459402?psc=1', 'https://www.amazon.com/All-New-Made-Amazon-Frame-Release/dp/B0CQ1KDRVP/ref=zg_bsnr_g_370783011_d_sccl_36/132-6980086-9459402?psc=1', 'https://www.amazon.com/All-New-Ring-Cameras-Generation-Spotlight/dp/B0DQ2GG2DP/ref=zg_bsnr_g_370783011_d_sccl_37/132-6980086-9459402?psc=1', 'https://www.amazon.com/All-New-Made-Amazon-Frame-Release/dp/B0CQ1RYFKN/ref=zg_bsnr_g_370783011_d_sccl_38/132-6980086-9459402?psc=1', 'https://www.amazon.com/Amazon-Keyboard-compatible-generation-release/dp/B0CKDP9HN1/ref=zg_bsnr_g_370783011_d_sccl_39/132-6980086-9459402?psc=1', 'https://www.amazon.com/Amazon-Kindle-Scribe-Magnetic-Protective/dp/B0D5KR2H2Y/ref=zg_bsnr_g_370783011_d_sccl_40/132-6980086-9459402?psc=1', 'https://www.amazon.com/All-New-Amazon-Universal-Large-Full-Motion/dp/B0DCT5574W/ref=zg_bsnr_g_370783011_d_sccl_41/132-6980086-9459402?psc=1', 'https://www.amazon.com/All-New-Limited-Amazon-Generations-Separately/dp/B0CZY2X5ND/ref=zg_bsnr_g_370783011_d_sccl_42/132-6980086-9459402?psc=1', 'https://www.amazon.com/All-New-Ring-Cameras-Generation-Spotlight/dp/B0DQ2F3Z1S/ref=zg_bsnr_g_370783011_d_sccl_43/132-6980086-9459402?psc=1', 'https://www.amazon.com/All-New-Ring-Stake-Cameras-Panels/dp/B0CG2HZ5W9/ref=zg_bsnr_g_370783011_d_sccl_44/132-6980086-9459402?psc=1', 'https://www.amazon.com/All-New-Made-Amazon-Frame-Release/dp/B0CQ1MRD79/ref=zg_bsnr_g_370783011_d_sccl_45/132-6980086-9459402?psc=1', 'https://www.amazon.com/4-Year-Protection-plan-Amazon-release/dp/B0CZB8J357/ref=zg_bsnr_g_370783011_d_sccl_46/132-6980086-9459402?psc=1', 'https://www.amazon.com/All-New-Amazon-Standing-Remote-Release/dp/B0D4P4Y8GX/ref=zg_bsnr_g_370783011_d_sccl_47/132-6980086-9459402?psc=1', 'https://www.amazon.com/Amazon-Fire-TV-50-Soundbar/dp/B0DSGL8J26/ref=zg_bsnr_g_370783011_d_sccl_48/132-6980086-9459402?psc=1', 'https://www.amazon.com/4-Year-Protection-plan-Amazon-release/dp/B0CZBCR9N7/ref=zg_bsnr_g_370783011_d_sccl_49/132-6980086-9459402?psc=1', 'https://www.amazon.com/4-Year-Protection-plan-Amazon-release/dp/B0CZBKM7ZC/ref=zg_bsnr_g_370783011_d_sccl_50/132-6980086-9459402?psc=1']\n",
      "|||||||||||||||||||||||||||| Data appended successfully with continued index. |||||||||||||||||||||||||||\n",
      "VISITING :  https://www.amazon.com/gp/new-releases/amazon-devices/370783011/ref=zg_bsnr_pg_2_amazon-devices?ie=UTF8&pg=2\n",
      "LINKS :  ['https://www.amazon.com/All-New-Amazon-Extension-Length-Releases/dp/B0DCKZGZCN/ref=zg_bsnr_g_370783011_d_sccl_1/133-0795283-7933804?psc=1', 'https://www.amazon.com/Echo-Bundle-release-Charcoal-Limited/dp/B0DKPG12X1/ref=zg_bsnr_g_370783011_d_sccl_2/133-0795283-7933804?psc=1', 'https://www.amazon.com/4-Year-Protection-plan-Amazon-Mini-LED/dp/B0D54XY3HB/ref=zg_bsnr_g_370783011_d_sccl_3/133-0795283-7933804?psc=1', 'https://www.amazon.com/All-New-Amazon-Standing-Remote-Release/dp/B0D4PLXTVC/ref=zg_bsnr_g_370783011_d_sccl_4/133-0795283-7933804?psc=1', 'https://www.amazon.com/Antiglare-Protector-Paperwhite-Releases-Colorsoft/dp/B0D693NPQJ/ref=zg_bsnr_g_370783011_d_sccl_5/133-0795283-7933804?psc=1', 'https://www.amazon.com/4-Year-Protection-plan-Amazon-Mini-LED/dp/B0C1TR8L7B/ref=zg_bsnr_g_370783011_d_sccl_6/133-0795283-7933804?psc=1', 'https://www.amazon.com/amazon-fire-tv-85-inch-omni-mini-led-series-with-fire-tv-soundbar-plus/dp/B0DDFQD5QS/ref=zg_bsnr_g_370783011_d_sccl_7/133-0795283-7933804?psc=1', 'https://www.amazon.com/3-Year-Protection-Plan-Echo-Show/dp/B0CF9K6K1M/ref=zg_bsnr_g_370783011_d_sccl_8/133-0795283-7933804?psc=1', 'https://www.amazon.com/All-New-Ring-Cameras-Outdoor-Spotlight/dp/B0DQ6HFK5Z/ref=zg_bsnr_g_370783011_d_sccl_9/133-0795283-7933804?psc=1', 'https://www.amazon.com/All-New-Ring-Cameras-Outdoor-Spotlight/dp/B0DQ6H2XL2/ref=zg_bsnr_g_370783011_d_sccl_10/133-0795283-7933804?psc=1', 'https://www.amazon.com/3-Year-Protection-Amazon-Kindle-Colorsoft/dp/B0DFZZ9ZTV/ref=zg_bsnr_g_370783011_d_sccl_11/133-0795283-7933804?psc=1', 'https://www.amazon.com/3-Year-Protection-Plan-Echo-Show/dp/B0C5F5WZTD/ref=zg_bsnr_g_370783011_d_sccl_12/133-0795283-7933804?psc=1', 'https://www.amazon.com/All-New-Adapter-Amazon-Generations-Generation/dp/B0CGL1Y2WX/ref=zg_bsnr_g_370783011_d_sccl_13/133-0795283-7933804?psc=1', 'https://www.amazon.com/3-Year-Protection-Kindle-Paper-White/dp/B0C8RZ6WZ5/ref=zg_bsnr_g_370783011_d_sccl_14/133-0795283-7933804?psc=1', 'https://www.amazon.com/Protection-Soundbar-subwoofer-surround-speakers/dp/B0DFJ8G4G2/ref=zg_bsnr_g_370783011_d_sccl_15/133-0795283-7933804?psc=1', 'https://www.amazon.com/3-Year-Accident-Protection-Kindle-Scribe/dp/B0CZ9TCLYV/ref=zg_bsnr_g_370783011_d_sccl_16/133-0795283-7933804?psc=1', 'https://www.amazon.com/amazon-fire-tv-75-inch-omni-mini-led-series-with-fire-tv-soundbar-plus/dp/B0DDFJP76P/ref=zg_bsnr_g_370783011_d_sccl_17/133-0795283-7933804?psc=1', 'https://www.amazon.com/4-Year-Protection-plan-Amazon-Mini-LED/dp/B0C1TQKY4G/ref=zg_bsnr_g_370783011_d_sccl_18/133-0795283-7933804?psc=1', 'https://www.amazon.com/1-Year-Protection-Plan-Echo-Show/dp/B0CF8WQDGL/ref=zg_bsnr_g_370783011_d_sccl_19/133-0795283-7933804?psc=1', 'https://www.amazon.com/2-Year-Protection-plan-Amazon-Soundbar/dp/B0DFHT1CJD/ref=zg_bsnr_g_370783011_d_sccl_20/133-0795283-7933804?psc=1', 'https://www.amazon.com/amazon-fire-tv-65-inch-omni-mini-led-series-with-fire-tv-soundbar-plus/dp/B0DDFM49PH/ref=zg_bsnr_g_370783011_d_sccl_21/133-0795283-7933804?psc=1', 'https://www.amazon.com/4-Year-Protection-plan-Amazon-Mini-LED/dp/B0C1TRJD8L/ref=zg_bsnr_g_370783011_d_sccl_22/133-0795283-7933804?psc=1', 'https://www.amazon.com/3-Year-Protection-Plan-for-Kindle/dp/B0CNXZ4J86/ref=zg_bsnr_g_370783011_d_sccl_23/133-0795283-7933804?psc=1', 'https://www.amazon.com/1-Year-Protection-Kindle-Paper-White/dp/B0C8S59XJL/ref=zg_bsnr_g_370783011_d_sccl_24/133-0795283-7933804?psc=1', 'https://www.amazon.com/1-Year-Protection-Plan-Echo-Show/dp/B0C5F5GCN3/ref=zg_bsnr_g_370783011_d_sccl_25/133-0795283-7933804?psc=1', 'https://www.amazon.com/amazon-fire-tv-55-inch-omni-mini-led-series-with-fire-tv-soundbar-plus/dp/B0DDFCZLN3/ref=zg_bsnr_g_370783011_d_sccl_26/133-0795283-7933804?psc=1', 'https://www.amazon.com/3-Year-Protection-plan-Amazon-Soundbar/dp/B0DFHS39YZ/ref=zg_bsnr_g_370783011_d_sccl_27/133-0795283-7933804?psc=1', 'https://www.amazon.com/2-Year-Protection-Amazon-Soundbar-Subwoofer/dp/B0DFHXKHTQ/ref=zg_bsnr_g_370783011_d_sccl_28/133-0795283-7933804?psc=1', 'https://www.amazon.com/Protection-Soundbar-subwoofer-surround-speakers/dp/B0DFJMHSGF/ref=zg_bsnr_g_370783011_d_sccl_29/133-0795283-7933804?psc=1', 'https://www.amazon.com/1-Year-Protection-Amazon-Kindle-Colorsoft/dp/B0DG1C7TH9/ref=zg_bsnr_g_370783011_d_sccl_30/133-0795283-7933804?psc=1', 'https://www.amazon.com/Replacement-Remote-DENON-DRA-295-Control/dp/B0DGL81YVT/ref=zg_bsnr_g_370783011_d_sccl_81/133-0795283-7933804?psc=1', 'https://www.amazon.com/Black-Screen-Protector-Essentials-Bundle/dp/B0DLR7VCQX/ref=zg_bsnr_g_370783011_d_sccl_82/133-0795283-7933804?psc=1', 'https://www.amazon.com/Fire-Lilac-Screen-Protector-Bundle/dp/B0DLR3K6Q7/ref=zg_bsnr_g_370783011_d_sccl_83/133-0795283-7933804?psc=1', 'https://www.amazon.com/3-Year-Protection-Amazon-Soundbar-Subwoofer/dp/B0DFHZYG8N/ref=zg_bsnr_g_370783011_d_sccl_84/133-0795283-7933804?psc=1', 'https://www.amazon.com/1-Year-Accident-Protection-Kindle-Scribe/dp/B0CZB7YRNT/ref=zg_bsnr_g_370783011_d_sccl_85/133-0795283-7933804?psc=1', 'https://www.amazon.com/Protection-Soundbar-subwoofer-surround-speakers/dp/B0DFHT5MLD/ref=zg_bsnr_g_370783011_d_sccl_86/133-0795283-7933804?psc=1', 'https://www.amazon.com/1-Year-Protection-plan-Amazon-Soundbar/dp/B0DFJF19FS/ref=zg_bsnr_g_370783011_d_sccl_87/133-0795283-7933804?psc=1', 'https://www.amazon.com/1-Year-Protection-Plan-for-Kindle/dp/B0CNY277KW/ref=zg_bsnr_g_370783011_d_sccl_88/133-0795283-7933804?psc=1']\n",
      "|||||||||||||||||||||||||||| Data appended successfully with continued index. |||||||||||||||||||||||||||\n",
      "VISITING :  https://www.amazon.com/gp/new-releases/amazon-devices/121475272011/ref=zg_bsnr_nav_amazon-devices_1\n",
      "NO PRODUCTS FOUND\n",
      "VISITING :  https://www.amazon.com/gp/new-releases/amazon-devices/121475272011/ref=zg_bsnr_nav_amazon-devices_1\n",
      "NO PRODUCTS FOUND\n",
      "VISITING :  https://www.amazon.com/gp/new-releases/amazon-devices/121475272011/ref=zg_bsnr_nav_amazon-devices_1\n"
     ]
    },
    {
     "ename": "InvalidSessionIdException",
     "evalue": "Message: invalid session id: session deleted as the browser has closed the connection\nfrom disconnected: Unable to receive message from renderer\n  (Session info: chrome=134.0.6998.178)\nStacktrace:\n\tGetHandleVerifier [0x00007FF788C64C25+3179557]\n\t(No symbol) [0x00007FF7888C88A0]\n\t(No symbol) [0x00007FF7887591CA]\n\t(No symbol) [0x00007FF78874664C]\n\t(No symbol) [0x00007FF78874633A]\n\t(No symbol) [0x00007FF788743EC2]\n\t(No symbol) [0x00007FF78874498F]\n\t(No symbol) [0x00007FF78875350E]\n\t(No symbol) [0x00007FF7887699B3]\n\t(No symbol) [0x00007FF788770AEA]\n\t(No symbol) [0x00007FF7887450FD]\n\t(No symbol) [0x00007FF7887691A1]\n\t(No symbol) [0x00007FF788800667]\n\t(No symbol) [0x00007FF7887D7A03]\n\t(No symbol) [0x00007FF7887A06D0]\n\t(No symbol) [0x00007FF7887A1983]\n\tGetHandleVerifier [0x00007FF788CC67CD+3579853]\n\tGetHandleVerifier [0x00007FF788CDD1D2+3672530]\n\tGetHandleVerifier [0x00007FF788CD2153+3627347]\n\tGetHandleVerifier [0x00007FF788A3092A+868650]\n\t(No symbol) [0x00007FF7888D2FFF]\n\t(No symbol) [0x00007FF7888CF4A4]\n\t(No symbol) [0x00007FF7888CF646]\n\t(No symbol) [0x00007FF7888BEAA9]\n\tBaseThreadInitThunk [0x00007FFEB6A4259D+29]\n\tRtlUserThreadStart [0x00007FFEB8A8AF38+40]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidSessionIdException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m driver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome(seleniumwire_options \u001b[38;5;241m=\u001b[39m options)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# driver = webdriver.Firefox()\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnextPage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m Previous_Height \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mexecute_script(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn document.body.scrollHeight\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# It will scroll to right above the footer of the page then scoll to the top then back to the bottom untill there is no new items being loaded\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ML\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:363\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    362\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ML\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:354\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    352\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    355\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\ML\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mInvalidSessionIdException\u001b[0m: Message: invalid session id: session deleted as the browser has closed the connection\nfrom disconnected: Unable to receive message from renderer\n  (Session info: chrome=134.0.6998.178)\nStacktrace:\n\tGetHandleVerifier [0x00007FF788C64C25+3179557]\n\t(No symbol) [0x00007FF7888C88A0]\n\t(No symbol) [0x00007FF7887591CA]\n\t(No symbol) [0x00007FF78874664C]\n\t(No symbol) [0x00007FF78874633A]\n\t(No symbol) [0x00007FF788743EC2]\n\t(No symbol) [0x00007FF78874498F]\n\t(No symbol) [0x00007FF78875350E]\n\t(No symbol) [0x00007FF7887699B3]\n\t(No symbol) [0x00007FF788770AEA]\n\t(No symbol) [0x00007FF7887450FD]\n\t(No symbol) [0x00007FF7887691A1]\n\t(No symbol) [0x00007FF788800667]\n\t(No symbol) [0x00007FF7887D7A03]\n\t(No symbol) [0x00007FF7887A06D0]\n\t(No symbol) [0x00007FF7887A1983]\n\tGetHandleVerifier [0x00007FF788CC67CD+3579853]\n\tGetHandleVerifier [0x00007FF788CDD1D2+3672530]\n\tGetHandleVerifier [0x00007FF788CD2153+3627347]\n\tGetHandleVerifier [0x00007FF788A3092A+868650]\n\t(No symbol) [0x00007FF7888D2FFF]\n\t(No symbol) [0x00007FF7888CF4A4]\n\t(No symbol) [0x00007FF7888CF646]\n\t(No symbol) [0x00007FF7888BEAA9]\n\tBaseThreadInitThunk [0x00007FFEB6A4259D+29]\n\tRtlUserThreadStart [0x00007FFEB8A8AF38+40]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"NewCategory.csv\")\n",
    "csv_file = \"Products.csv\"\n",
    "first = False\n",
    "# Extract the link from the second column and add the array to the third column\n",
    "for _, row in df.iterrows():\n",
    "    category = row[0]  # Assuming the first column contains the category name\n",
    "    productLinks = row[2]  # Assuming the second column contains the link\n",
    "    # print(\"PRODUCT LINKS : \", productLinks)\n",
    "    options = {\n",
    "    'headers' : {\n",
    "        \"User-Agent\": UserAgent().random\n",
    "        }\n",
    "    }\n",
    "    for link in productLinks.split(', '):\n",
    "        nextPage = link\n",
    "        while nextPage:\n",
    "            print(\"VISITING : \", nextPage)\n",
    "            driver = webdriver.Chrome(seleniumwire_options = options)\n",
    "            # driver = webdriver.Firefox()\n",
    "            driver.get(nextPage)\n",
    "            Previous_Height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            while True:\n",
    "                # It will scroll to right above the footer of the page then scoll to the top then back to the bottom untill there is no new items being loaded\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight - 1200);\")\n",
    "                time.sleep(5)\n",
    "                driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "                time.sleep(5)\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight - 1200);\")\n",
    "                time.sleep(5)\n",
    "                New_Height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                if New_Height == Previous_Height:\n",
    "                    break\n",
    "                Previous_Height = New_Height\n",
    "                # if there is a popup press the close button\n",
    "                try:\n",
    "                    driver.find_element_by_class_name('modal-close').click()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            html = driver.page_source\n",
    "            driver.quit()\n",
    "            soup = bsp(html, 'lxml')\n",
    "            board = soup.find('div', class_ = 'p13n-gridRow _cDEzb_grid-row_3Cywl')\n",
    "            if not board:\n",
    "                print(\"NO PRODUCTS FOUND\")\n",
    "                continue\n",
    "            products = board.find_all('div', class_ = 'p13n-sc-uncoverable-faceout')\n",
    "            linkers = [product.find('a') for product in products]\n",
    "            links = []\n",
    "            for haha in linkers:\n",
    "                try:\n",
    "                    links.append(base + haha['href'])\n",
    "                except:\n",
    "                    pass\n",
    "            print(\"LINKS : \", links)\n",
    "            info = {\"Categories\": category, \"Products\": links}\n",
    "            new_df = pd.DataFrame(info)\n",
    "            if not first:\n",
    "                new_df.to_csv(csv_file, mode='a', index = False)\n",
    "                first = True\n",
    "            else:\n",
    "                new_df.to_csv(csv_file, mode='a', header=False, index = False)\n",
    "            print(\"|||||||||||||||||||||||||||| Data appended successfully with continued index. |||||||||||||||||||||||||||\")\n",
    "            nextPage = soup.find('li', class_ = 'a-last')\n",
    "            if nextPage:\n",
    "                try:\n",
    "                    nextPage = base + nextPage.find('a')['href']\n",
    "                except:\n",
    "                    nextPage = None\n",
    "            else:\n",
    "                nextPage = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file, skipping the header\n",
    "file_path = \"AmazonCategory.csv\"\n",
    "df = pd.read_csv(file_path, delimiter=\";\", header=None, skiprows=1)\n",
    "\n",
    "# Create the dictionary with subcategories as keys and categories as values\n",
    "category = {}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    dawcategory, subcategory = row[0], row[1]\n",
    "    \n",
    "    if pd.notna(subcategory):  # Ignore empty subcategories\n",
    "        category[subcategory] = dawcategory\n",
    "\n",
    "# Print the dictionary\n",
    "for subcategory, dawcategory in category.items():\n",
    "    print(f\"Subcategory: {subcategory} → Category: {dawcategory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session = requests.Session()\n",
    "# session.headers.update({\"User-Agent\": UserAgent().random})\n",
    "driver = webdriver.Chrome()\n",
    "# driver = webdriver.Firefox(service = service, options = options)\n",
    "driver.get(base)\n",
    "#   It will scroll to right above the footer of the page then scoll to the top then back to the bottom untill there is no new items being loaded\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(10) \n",
    "html = driver.page_source\n",
    "driver.quit()          \n",
    "soup = bsp(html, 'lxml')\n",
    "boxes = soup.find_all('a', class_ = 'a-link-normal _fluid-quad-image-label-v2_style_centerImage__30wh- aok-block image-window')\n",
    "print(boxes)\n",
    "links = []\n",
    "urls = []\n",
    "categ = []\n",
    "for box in boxes:\n",
    "    actual = box.get('href')\n",
    "    url = base + actual\n",
    "    print(\"Url : \", url)\n",
    "    urls.append(url)\n",
    "    som = box.get('aria-label')\n",
    "    link = som.replace(' ', '+')\n",
    "    link = base + thirdWheel + link\n",
    "    print(\"Links : \", link)\n",
    "    links.append(link)\n",
    "    categ.append(som)\n",
    "    print(som)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = {}\n",
    "for cat in category:\n",
    "    url = base+thirdWheel+cat\n",
    "    urls[url] = category[cat]\n",
    "print(urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_proxies():\n",
    "    proxy_list_url = 'https://free-proxy-list.net/'\n",
    "    response = requests.get(proxy_list_url)\n",
    "    soup = bsp(response.text, 'html.parser')\n",
    "    proxy_data = []\n",
    "    rows = soup.find_all('tr')[1:]\n",
    "    for row in rows:\n",
    "        columns = row.find_all('td')\n",
    "        if len(columns) >= 8:\n",
    "            ip_address = columns[0].text.strip()\n",
    "            google_enabled = columns[5].text.strip().lower() == 'yes'\n",
    "            https_enabled = columns[6].text.strip().lower() == 'yes'\n",
    "            last_checked = columns[7].text.strip()\n",
    "            if (last_checked.endswith('mins ago') and int(last_checked.split(' ')[0]) < 15) or last_checked.endswith('hours ago'):\n",
    "                if google_enabled or https_enabled:\n",
    "                    proxy_data.append({'ip_address': ip_address, 'google_enabled': google_enabled, 'https_enabled': https_enabled})\n",
    "\n",
    "    return proxy_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_user_agent(proxy):\n",
    "    if proxy:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "            'http': f'http://{proxy}',\n",
    "            'https': f'https://{proxy}'\n",
    "        }\n",
    "    else:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "        }\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = []\n",
    "# categories = []\n",
    "cat = 0\n",
    "me = False\n",
    "# proxies = get_valid_proxies()\n",
    "\n",
    "for url, cat in urls.items():\n",
    "    total_samples = 0\n",
    "    myurl = url\n",
    "    while myurl:\n",
    "        # proxy = proxies[total_samples % len(proxies)] if proxies else None\n",
    "        options = {\n",
    "            'headers' : {\n",
    "                \"User-Agent\": UserAgent().random\n",
    "            }\n",
    "        }\n",
    "        print(\"VISITING : \", myurl)\n",
    "        # sdsd = Options()\n",
    "        # sdsd.add_argument(\"--headless\")  # Run in background\n",
    "        # sdsd.add_argument(\"--disable-gpu\")\n",
    "        # sdsd.add_argument(\"--no-sandbox\")\n",
    "        # sdsd.add_argument(\"--disable-dev-shm-usage\")\n",
    "        driver = webdriver.Chrome(seleniumwire_options = options)\n",
    "        # driver = webdriver.Firefox()\n",
    "        driver.get(myurl)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "        time.sleep(10)\n",
    "        html = driver.page_source\n",
    "        driver.quit()\n",
    "        # headers = rotate_user_agent(proxy)\n",
    "        # response = req.get(myurl, headers = headers)\n",
    "        # session = requests.Session()\n",
    "        # session.headers.update({\"User-Agent\": UserAgent().random})\n",
    "        # response = req.get(myurl, headers = {\"User-Agent\": UserAgent().random})\n",
    "        # if response.status_code == 200:\n",
    "            # soup = bsp(response.text, 'lxml')\n",
    "        soup = bsp(html, 'lxml')\n",
    "        # print(soup)\n",
    "        productes = soup.find_all('a', class_ = 'a-link-normal s-no-outline')\n",
    "        if not productes:\n",
    "            productes = soup.find_all('a', class_ = 'a-link-normal s-line-clamp-4 s-link-style a-text-normal')\n",
    "        if not productes:\n",
    "            productes = soup.find_all('a' , class_= 'a-link-normal s-line-clamp-2 s-link-style a-text-normal')\n",
    "        print(productes)\n",
    "        for product in productes:\n",
    "            product = base + product.get('href')\n",
    "            products.append(product)\n",
    "            print(product)\n",
    "        csv_file = 'Amazon.csv'\n",
    "        cat = cat.replace('+', \" \")\n",
    "        info = {\"Products\": products, \"Categories\": cat}\n",
    "        new_df = pd.DataFrame(info)\n",
    "        if not me:\n",
    "            new_df.to_csv(csv_file, mode='a', index = False)\n",
    "            me = True\n",
    "        else:\n",
    "            new_df.to_csv(csv_file, mode='a', header=False, index = False)\n",
    "        print(\"|||||||||||||||||||||||||||| Data appended successfully with continued index. |||||||||||||||||||||||||||\")\n",
    "        \n",
    "        total_samples += 1\n",
    "        # Reset lists to prevent duplicates in the next iteration\n",
    "        products.clear()\n",
    "        nextP = soup.find('a', class_ = 's-pagination-item s-pagination-next s-pagination-button s-pagination-button-accessibility s-pagination-separator')\n",
    "        if nextP:\n",
    "            myurl = base + nextP.get('href')\n",
    "        else:\n",
    "            break\n",
    "    cat += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csv_file = 'Amazon.csv'\n",
    "meta = 'Products.csv'\n",
    "rev = 'Reviews.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "samples = 0\n",
    "me = False\n",
    "for _,row in df.iterrows():\n",
    "    agent = {\n",
    "        \"link\" : \"\",\n",
    "        \"date\" : \"\",\n",
    "        \"main_category\" : \"\",\n",
    "        \"title\" : \"\",\n",
    "        \"average_rating\" : 0.0,\n",
    "        \"rating_number\" : 0,\n",
    "        \"features\":[],\n",
    "        \"description\":[],\n",
    "        \"price\" : \"\",\n",
    "        \"images\":[],\n",
    "        \"videos\":[],\n",
    "        \"store\" : \"\",\n",
    "        \"categories\" : \"\",\n",
    "        \"details\":{},\n",
    "        \"parent_asin\" : \"\",\n",
    "        \"bought_together\": []\n",
    "    }\n",
    "\n",
    "    comment = {\n",
    "        \"link\" : \"\",\n",
    "        \"rating\" : 0.0,\n",
    "        \"title\" : \"\",\n",
    "        \"text\" : \"\",\n",
    "        \"images\" : \"\",\n",
    "        \"asin\" : \"\",\n",
    "        \"parent_asin\" : \"\",\n",
    "        \"user_id\" : \"\",\n",
    "        \"timestamp\" : \"\",\n",
    "        \"verified_purchase\" : 0,\n",
    "        \"helpful_vote\" : \"\"\n",
    "    }\n",
    "    print(\"\\n\\n\\n\\n\\n\\n-------------------------------------------------------------------Product-------------------------------------------------------------------\")\n",
    "    options = {\n",
    "            'headers' :{\n",
    "                \"User-Agent\":UserAgent().random\n",
    "            }\n",
    "        }\n",
    "    link = row['Products']\n",
    "    print(link)\n",
    "    driver = webdriver.Chrome(seleniumwire_options = options)\n",
    "    driver.get(link)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "    time.sleep(10)\n",
    "    html = driver.page_source\n",
    "    driver.quit()\n",
    "    soup = bsp(html, 'lxml')\n",
    "\n",
    "    feat = []\n",
    "    img = []\n",
    "    vid = []\n",
    "    det = []\n",
    "\n",
    "    agent[\"link\"] = link\n",
    "\n",
    "\n",
    "    # extracting Category\n",
    "    category = row['Categories']\n",
    "    print(\"Category :\", category)\n",
    "    agent[\"main_category\"] = category\n",
    "\n",
    "    # extracting Title\n",
    "    try:\n",
    "        titleBoard = soup.find('span', id = 'productTitle')\n",
    "        title = titleBoard.text.strip()\n",
    "    except:\n",
    "        title = \"\"\n",
    "    print(\"Title :\", title)\n",
    "    agent[\"title\"] = title\n",
    "\n",
    "    # extracting ratings\n",
    "    try:\n",
    "        ratingBoard = soup.find('div', id = 'averageCustomerReviews')\n",
    "        # extracting average rating\n",
    "        avgRatingBoard = ratingBoard.find('span', class_ = 'a-size-base a-color-base')\n",
    "        avgRating = avgRatingBoard.text.strip()\n",
    "        # extracting rating number\n",
    "        ratingNoBoard = ratingBoard.find('span', id = 'acrCustomerReviewText')\n",
    "        ratingNo = ratingNoBoard.text.strip()\n",
    "        junk = \" ratings\"\n",
    "        ratingNo = ratingNo.replace(junk, \"\")\n",
    "    except:\n",
    "        avgRating = \"\"\n",
    "        ratingNo = \"\"\n",
    "    print(\"Average Rating :\", avgRating)\n",
    "    agent[\"average_rating\"] = avgRating\n",
    "    print(\"Rating :\", ratingNo)\n",
    "    agent[\"rating_number\"] = ratingNo\n",
    "\n",
    "    # extracting price\n",
    "    try:\n",
    "        priceBoard = soup.find('div', class_ = 'a-section a-spacing-none aok-align-center aok-relative').find('span', class_ = 'aok-offscreen')\n",
    "        price = priceBoard.text.strip()[0:5]\n",
    "    except:\n",
    "        price = \"\"\n",
    "    print(\"Price :\", price)\n",
    "    agent[\"price\"] = price\n",
    "\n",
    "    # exracting media\n",
    "    try:\n",
    "        mediaBoard = soup.find('ul', class_ = 'a-unordered-list a-nostyle a-button-list a-vertical a-spacing-top-micro regularAltImageViewLayout')\n",
    "        if not mediaBoard:\n",
    "            mediaBoard = soup.find('ul', class_ = 'a-unordered-list a-nostyle a-button-list a-vertical a-spacing-top-micro gridAltImageViewLayoutIn1x7')\n",
    "            if not mediaBoard:\n",
    "                mediaBoard = soup.find('ul', class_ = 'a-unordered-list a-nostyle a-button-list a-vertical a-spacing-top-extra-large regularAltImageViewLayout')\n",
    "        media = mediaBoard.find_all('img')\n",
    "        images = [img['src'] for img in media]\n",
    "        vids = images[-1]\n",
    "        images = images[0:-1]\n",
    "    except:\n",
    "        images = []\n",
    "        vids = []\n",
    "    print(\"Images :\", images)\n",
    "    agent[\"images\"] = images   \n",
    "    print(\"Videos :\", vids)\n",
    "    agent[\"videos\"] = vids\n",
    "\n",
    "    # extracting the store\n",
    "    try:\n",
    "        storeBoard = soup.find('tr', class_ = 'a-spacing-small po-brand').find('td', class_ = 'a-span9')\n",
    "        store = storeBoard.text.strip()\n",
    "    except:\n",
    "        try:\n",
    "            storeBoard = soup.find('a', id = 'bylineInfo').text.strip()\n",
    "            junk = \"Visit the \"\n",
    "            store = storeBoard.replace(junk, \"\")\n",
    "        except:\n",
    "            store = \"\"\n",
    "    print(\"Store :\", store)\n",
    "    agent[\"store\"] = store\n",
    "\n",
    "    # extracting features\n",
    "    try:\n",
    "        featuresBoard = soup.find('ul', class_ = 'a-unordered-list a-vertical a-spacing-mini').find_all('li')\n",
    "        features = [feature.text.strip() for feature in featuresBoard]\n",
    "    except:\n",
    "        features = []\n",
    "    print(\"Features :\", features)\n",
    "    agent[\"features\"] = features\n",
    "        \n",
    "    # extracting details\n",
    "    try:\n",
    "        detailsBoard = soup.find('div', class_ = 'a-expander-content a-expander-section-content a-section-expander-inner').find('table', class_ = 'a-keyvalue prodDetTable').find_all('tr')\n",
    "    except:\n",
    "        try:\n",
    "            detailsBoard = soup.find('div', id = 'detailBullets_feature_div').find_all('li')\n",
    "        except:\n",
    "            try:\n",
    "                detailsBoard = soup.find('table', id = 'productDetails_detailBullets_sections1').find_all('tr')\n",
    "            except:   \n",
    "                try:\n",
    "                    detailsBoard = soup.find('ul', class_ = 'a-unordered-list a-nostyle a-vertical a-spacing-none detail-bullet-list').find_all('tr')\n",
    "                except:\n",
    "                    detailsBoard = []\n",
    "\n",
    "    # find th for the dictionary subject and td for the disctionary details\n",
    "    details = {}\n",
    "    if detailsBoard:\n",
    "        for detailBoard in detailsBoard:\n",
    "            detailingH = detailBoard.find('th').text.strip() if detailBoard.find('th') else None\n",
    "            detailingA = detailBoard.find('td').text.strip() if detailBoard.find('td') else None\n",
    "            if detailingH and detailingA:\n",
    "                detailed = {detailingH: detailingA} \n",
    "                details |= detailed  \n",
    "    else:\n",
    "        details = {}\n",
    "    print(\"Details :\", details)\n",
    "    agent[\"details\"] = details\n",
    "                    \n",
    "    # extracting asin\n",
    "    try:\n",
    "        asin = details['ASIN']\n",
    "    except:\n",
    "        asin = \"\"\n",
    "    print(\"ASIN :\", asin)\n",
    "    agent[\"parent_asin\"] = asin\n",
    "\n",
    "    try:\n",
    "        date = details['Date First Available']\n",
    "    except:\n",
    "        date = \"\"\n",
    "    print(\"Date :\", date)\n",
    "    agent[\"date\"] = date\n",
    "    \n",
    "    # extracting description\n",
    "    try:\n",
    "        descriptionBoard = soup.find('table', class_ = 'a-normal a-spacing-micro')\n",
    "        if not descriptionBoard:\n",
    "            descriptionBoard = soup.find('ul', id = 'a-nostyle').find_all('div', class_ = 'a-fixed-left-grid-col a-col-right')\n",
    "        descriptions = descriptionBoard.find_all('td', class_ = 'a-span9')\n",
    "        description = [dec.text.strip() for dec in descriptions]\n",
    "    except:\n",
    "        description = []\n",
    "    print(\"Description :\", description)\n",
    "    agent[\"description\"] = description\n",
    "\n",
    "    # extracting bought together\n",
    "    try:\n",
    "        boughtTogetherBoard = soup.find('div', id = 'a-cardui _c3AtZ_new-thumbnail-box_1W9Ku _c3AtZ_two-item-thumbnail-box_7kF95')\n",
    "        if not boughtTogetherBoard:\n",
    "            boughtTogetherBoard = soup.find('div', class_ = 'a-cardui _p13n-desktop-sims-fbt_fbt-desktop_new-thumbnail-box__36bD3 _p13n-desktop-sims-fbt_fbt-desktop_two-item-thumbnail-box__jV2am')\n",
    "        if not boughtTogetherBoard:\n",
    "            boughtTogetherBoard = soup.find('div', class_ = 'a-cardui _p13n-desktop-sims-fbt_fbt-desktop_new-thumbnail-box__36bD3')\n",
    "        boughtTogetherS = boughtTogetherBoard.find_all('a', class_ = 'a-link-normal a-text-normal', href = True)\n",
    "        if not boughtTogetherS:\n",
    "            boughtTogetherS = boughtTogetherBoard.find_all('a', class_ = 'a-link-normal', href = True)        \n",
    "        boughtTogether = [base + link['href'] for link in boughtTogetherS]\n",
    "    except:\n",
    "        boughtTogether = []\n",
    "    print(\"Bought Together :\", boughtTogether)\n",
    "    agent[\"bought_together\"] = boughtTogether\n",
    "\n",
    "    # extracting further categories\n",
    "    try:\n",
    "        categoriesBoard = soup.find_all('div', id= 'tp-inline-twister-dim-values-container')\n",
    "        categoriesBoardAh = categoriesBoard[-1].find_all('li')\n",
    "        categories = [categoriesBoardh.text.strip() for categoriesBoardh in categoriesBoardAh]\n",
    "    except:\n",
    "        categories = []\n",
    "    print(\"Categories :\", categories)\n",
    "    agent[\"categories\"] = categories\n",
    "\n",
    "\n",
    "    ############################################################################################################################################################\n",
    "    print(\"\\n\\n\\n-------------------------------------------------------------------Comments-------------------------------------------------------------------\")\n",
    "    # extracting reviews\n",
    "    reviewsUS = soup.find('ul', id = 'cm-cr-dp-review-list').find_all('li')\n",
    "    reviewsGlobal = soup.find('ul', id = 'cm-cr-global-review-list').find_all('li')\n",
    "    reviews = reviewsUS + reviewsGlobal\n",
    "    for review in reviews:\n",
    "        # extracting rating\n",
    "        try:\n",
    "            reviewRatingBoard = review.find('i', {'data-hook': 'review-star-rating'}).find('span', class_ = 'a-icon-alt')\n",
    "            reviewRating = reviewRatingBoard.text[0:3]\n",
    "        except:\n",
    "            reviewRating = \"\"\n",
    "        print(\"Rating :\", reviewRating)\n",
    "        comment[\"rating\"].append(reviewRating)\n",
    "\n",
    "        # extracting title\n",
    "        try:\n",
    "            reviewTitleBoard = review.find('a', class_ = 'a-size-base a-link-normal review-title a-color-base review-title-content a-text-bold')\n",
    "            reviewTitles = reviewTitleBoard.find_all('span')\n",
    "            reviewTitle = reviewTitles[-1].text.strip()\n",
    "        except:\n",
    "            reviewTitle = \"\"\n",
    "        print(\"Title :\", reviewTitle)\n",
    "        comment[\"title\"].append(reviewTitle)\n",
    "\n",
    "        # extracting Text\n",
    "        try:\n",
    "            reviewTextBoard = review.find('div', class_ = 'a-expander-content reviewText review-text-content a-expander-partial-collapse-content')\n",
    "            reviewText = reviewTextBoard.text.strip()\n",
    "        except:\n",
    "            reviewText = \"\"\n",
    "        print(\"Review :\", reviewText)\n",
    "        comment[\"text\"].append(reviewText)\n",
    "\n",
    "        # extracting images\n",
    "        try:\n",
    "            reviewImages = review.find('div', class_ = 'review-image-tile-section').find_all('img')\n",
    "            Images = [img['src'] for img in reviewImages]\n",
    "        except:\n",
    "            Images = []\n",
    "        print(\"Images :\", Images)\n",
    "        comment[\"images\"].append(Images)\n",
    "\n",
    "        # extracting ASIN\n",
    "        reviewAsin = review.get('id')\n",
    "        print(\"ASIN :\", reviewAsin)\n",
    "        comment[\"asin\"].append(reviewAsin)\n",
    "\n",
    "        # extracting parent ASIN\n",
    "        reviewParentAsin = asin\n",
    "        print(\"Parent ASIN :\", reviewParentAsin)\n",
    "        comment[\"parent_asin\"].append(review)\n",
    "\n",
    "        # eaxtracting username\n",
    "        try:\n",
    "            username = review.find('span', class_ = 'a-profile-name').text.strip()\n",
    "        except:\n",
    "            username = \"\"\n",
    "        print(\"Username :\", username)\n",
    "        comment[\"user_id\"].append(username)\n",
    "\n",
    "        # extracting date\n",
    "        try:\n",
    "            reviewTimeBoard = review.find('span', class_ = 'a-size-base a-color-secondary review-date')\n",
    "            reviewTimes = reviewTimeBoard.text\n",
    "            reviewTimes = re.search(date_pattern, reviewTimes)\n",
    "            reviewTiming = datetime.strptime(reviewTimes, \"%B %d, %Y\")\n",
    "            reviewTime = int(reviewTiming.timestamp())\n",
    "        except:\n",
    "            reviewTime = \"\"\n",
    "        print(\"Date :\", reviewTime)\n",
    "        comment[\"timestamp\"].append(reviewTime)\n",
    "\n",
    "        verified = False\n",
    "        if review.find('span', class_ = 'a-size-mini a-color-state a-text-bold'):\n",
    "            verified = True\n",
    "        print(\"Verified :\", verified)\n",
    "        comment[\"verified_purchase\"].append(verified)\n",
    "\n",
    "        # extracting helpfulness\n",
    "        try:\n",
    "            reviewHelpful = review.find('span', class_ = 'a-size-base a-color-tertiary cr-vote-text')\n",
    "            reviewHelpful = reviewHelpful.text\n",
    "            haha = \"One\"\n",
    "            helpful = 0\n",
    "            Helpfulness = re.search(number_pattern, reviewHelpful)\n",
    "            if Helpfulness:\n",
    "                helpful = int(Helpfulness.group())\n",
    "            elif haha in reviewHelpful:\n",
    "                helpful = 1\n",
    "        except:\n",
    "            helpful = 0\n",
    "        print(\"Helpful :\", helpful)\n",
    "        comment[\"helpful_vote\"].append(helpful)\n",
    "\n",
    "        myMeta = pd.DataFrame(agent)\n",
    "        myRev = pd.DataFrame(comment)\n",
    "    if not me:\n",
    "        myMeta.to_csv(meta, mode='a', index = False)\n",
    "        myRev.to_csv(rev, mode='a', index = False)\n",
    "        me = True\n",
    "    else:\n",
    "        myMeta.to_csv(meta, mode='a', header=False, index = False)\n",
    "        myRev.to_csv(rev, mode='a', header=False, index = False)\n",
    "    \n",
    "    print(\"|||||||||||||||||||||||||||| Data appended successfully with continued index. |||||||||||||||||||||||||||\")\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the date strings\n",
    "date_str1 = \"June 22, 2016\"\n",
    "date_str2 = \"September 30, 2023\"\n",
    "\n",
    "# Convert the date strings to datetime objects\n",
    "date1 = datetime.strptime(date_str1, \"%B %d, %Y\")\n",
    "date2 = datetime.strptime(date_str2, \"%B %d, %Y\")\n",
    "\n",
    "# Compare the dates\n",
    "if date1 > date2:\n",
    "    print(f\"{date_str1} is greater than {date_str2}\")\n",
    "else:\n",
    "    print(f\"{date_str1} is not greater than {date_str2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date string in the format \"Month Day, Year\"\n",
    "date_str = \"January 17, 2025\"\n",
    "\n",
    "# Parse the string into a datetime object\n",
    "date_obj = datetime.strptime(date_str, \"%B %d, %Y\")\n",
    "\n",
    "# Convert to Unix time\n",
    "unix_time = int(date_obj.timestamp())\n",
    "\n",
    "print(f\"Unix Time: {unix_time}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example statement\n",
    "statement = \"The item costs 700 dollars, and the discount is.\"\n",
    "\n",
    "# Regular expression to match one or more digits\n",
    "number_pattern = r\"\\d+\"\n",
    "\n",
    "# Find the first number in the statement\n",
    "match = re.search(number_pattern, statement)\n",
    "\n",
    "if match:\n",
    "    number = int(match.group())  # Convert the matched number to an integer\n",
    "    print(f\"Extracted number: {number}\")\n",
    "else:\n",
    "    print(\"No number found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
